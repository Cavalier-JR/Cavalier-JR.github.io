<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="神经网络,深度学习,机器学习,蒲公英书">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Fetyloi&#39;s 《神经网络与深度学习》 学习笔记 | Fetyloi&#39;s Blog</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

    <script src="https://fastly.jsdelivr.net/npm/live2d-widgets@0/autoload.js"></script>
<meta name="generator" content="Hexo 7.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Fetyloi's Blog" type="application/atom+xml">
</head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Fetyloi&#39;s Blog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="" class="waves-effect waves-light">

      
      <i class="fas fa-list" style="zoom: 0.6;"></i>
      
      <span>网址导航</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a target="_blank" rel="noopener" href="https://www.bing.com">
          
          <span>New Bing</span>
        </a>
      </li>
      
      <li>
        <a target="_blank" rel="noopener" href="https://chat.openai.com">
          
          <span>OpenAi</span>
        </a>
      </li>
      
      <li>
        <a href="/navigate">
          
          <span>Navigate</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a class="modal-trigger waves-effect waves-light" onclick="switchNightMode()" title="深浅主题切换" >
      <i id="sum-moon-icon" class="fas fa-moon" style="zoom: 0.7;"></i>
    </a>
  </li>
</ul>




<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Fetyloi&#39;s Blog</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-list"></i>
			
			网址导航
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a target="_blank" rel="noopener" href="https://www.bing.com " style="margin-left:75px">
				  
		          <span>New Bing</span>
                  </a>
                </li>
              
                <li>

                  <a target="_blank" rel="noopener" href="https://chat.openai.com " style="margin-left:75px">
				  
		          <span>OpenAi</span>
                  </a>
                </li>
              
                <li>

                  <a href="/navigate " style="margin-left:75px">
				  
		          <span>Navigate</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Cavalier-JR" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>嘻嘻
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Cavalier-JR" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="嘻嘻" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/../images/11.png')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Fetyloi&#39;s 《神经网络与深度学习》 学习笔记</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 135px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
                                <span class="chip bg-color">神经网络</span>
                            </a>
                        
                            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">深度学习</span>
                            </a>
                        
                            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">机器学习</span>
                            </a>
                        
                            <a href="/tags/%E8%92%B2%E5%85%AC%E8%8B%B1%E4%B9%A6/">
                                <span class="chip bg-color">蒲公英书</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-category">
                                学习笔记
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    73 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <p><font color="blue"><strong>附上我的语雀文档链接吧，导出的.md文件貌似不太好使：</strong></font><br><a target="_blank" rel="noopener" href="https://www.yuque.com/yuqueyonghucoit3e/wefx9h/dlk9gxsk9fbg91uq?singleDoc#">https://www.yuque.com/yuqueyonghucoit3e/wefx9h/dlk9gxsk9fbg91uq?singleDoc#</a> 《《神经网络与深度学习》（邱锡鹏）》 密码：bz42<br>第1章 绪论</p>
<h2 id="tRQk9">1.表示学习</h2>
①表示学习：自动将输入信息转换为有效的特征，提高最终模型性能。

<p>②语义鸿沟：输入数据的底层特征和高层语义信息之间的不一致性和差异性。</p>
<p>③嵌入：用神经网络将高维局部表示空间映射到非常低维的分布式表示空间，并尽可能保持不同对象间的拓扑关系。</p>
<h2 id="HbCi1">2.深度学习</h2>
①深度学习：原始数据特征进行多步特征转换，得到特征表示，进一步输入到预测函数，得到最终结果。

<p>②深度：原始数据进行非线性特征转换的次数。</p>
<h2 id="EkK7z">3.赫布型学习</h2>
赫布型学习：俩神经元总相关联地受到刺激，使得它们之间的突触强度增加。

<h1 id="Ea6dm">第2章 机器学习概述</h1>
<h2 id="JZqGp">1.机器学习三要素</h2>
模型、学习准则、优化算法

<h2 id="N0xH1">2.线性模型与非线性模型</h2>
线性模型的假设空间为一个参数化的线性函数族；

<p>广义的非线性模型可以写为多个非线性基函数的线性组合。</p>
<h2 id="UMZEI">3.损失函数</h2>
&gt; 0-1损失；平方损失（不适用分类）；交叉熵损失（一般用于分类）；Hinge损失函数
&gt;

<h2 id="eQAeE">4.优化问题与优化算法</h2>
<h3 id="EA07U">优化问题与优化算法</h3>
优化问题：参数优化、超参数优化

<p>优化算法：梯度下降、提前停止、随机梯度下降、小批量梯度下降</p>
<p>（.提前停止：在快要发生过拟合的时候停止迭代——使用验证集，在验证集上错误率不再下降就停止迭代）</p>
<h3 id="vIfQg">超参数</h3>
超参数：定义模型结构或优化策略的一类参数

<p>常见超参数：聚类中类别个数、梯度下降中学习率/步长，正则化项系数，神经网络层数，SVM核函数</p>
<h2 id="tjxc4">5.参数估计方法</h2>
&gt; 经验风险最小化、结构风险最小化、最大似然估计、最大后验估计
&gt;

<p>经验风险最小化–&gt;过拟合–&gt;结构风险最小化–&gt;正则化</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1730373430149-accedfba-3c10-44eb-8311-4b83b1352b11.png"></p>
<h2 id="GikeT">6.偏差-方差分解</h2>
偏差-方差分解的目的是：在模型拟合能力和复杂度之间取得较好平衡

<p>降低偏差：增加数据特征、提高模型复杂度、减小正则化系数</p>
<p>降低方差：降低模型复杂度、加大正则化系数、引入先验、集成模型</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1730373401347-fa1537ac-88f1-41f2-ac87-8e9ff167a8d3.png"></p>
<h2 id="F3MbP">7.数据的特征表示</h2>
&gt; 图像特征、文本特征
&gt;

<h3 id="gvR7J">图像特征表示：每一维的值为图像中对应像素的灰度值</h3>
<h3 id="jZfVf">文本特征表示——词袋模型：</h3>
①思想：向量表示，向量维度为与词表大小相同，向量第i维值为1表示词表第i个词在向量中出现

<p>②缺点：不考虑词序，不能精确表示文本信息</p>
<p>③改进：使用N元特征——每N个连续词构成一个基本单元再放进词袋模型</p>
<h2 id="pAlno">8.传统特征学习【亦称维数约简/降维】</h2>
&gt; **特征选择、特征抽取**
&gt;

<h3 id="eWlhj">特征选择：保留有用的、去除没用的，方法有子集搜索、正则化等：</h3>
①子集搜索：

<p>  i.过滤式方法：每次增加最有信息量（信息增益高）/删除最没信息量（信息增益低）的特征</p>
<p>  ii.包裹式方法：每次增加对模型预测效果提升最有用/删除对模型预测效果提升最没用的特征</p>
<p>【从空增加是前向搜索，从原始特征集合删是反向搜索】</p>
<p>②正则化：会导致稀疏特征，因此间接实现了特征选择</p>
<h3 id="Ffvnd">特征抽取：原始特征投影到新的特征空间得到新表示，方法有监督和无监督：</h3>
①监督方法：eg.线性判别分析LDA

<p>②无监督方法：eg.主成分分析PCA；自编码器AE</p>
<h2 id="QSova">9.指标</h2>
准确率、错误率、精确率（查准率）、召回率（查全率）、F值、宏平均、微平均、交叉验证。。。

<h2 id="uFh7Z">10.理论和定理</h2>
&gt; PAC学习理论（可能近似正确学习理论）、没有免费的午餐定理、奥卡姆剃刀原理、丑小鸭定理、归纳偏置（贝叶斯中的先验）
&gt;

<p>①可能近似正确学习理论：就是模型泛化错误小到一个差不多的程度ε就行，而且能以一定概率1-δ（也就是有可能）学到这个模型。从而得出了一个公式描述，如果固定了上面所说的ε和δ，可以得出学习需要的样本数量。</p>
<p>②没有免费的午餐定理：任何算法都有局限性，如果算法对某个问题有效，那么对一些其他问题一定比纯随机算法要差</p>
<p>③奥卡姆剃刀原理：如果俩模型性能相近，选简单的那个</p>
<h1 id="EAfSR">第3章 线性模型</h1>
<h2 id="IInLg">1.Logistic回归（逻辑斯谛回归/对数几率回归）</h2>
Logistic回归采用交叉熵作为损失函数，可用梯度下降法、牛顿法等来优化参数

<h2 id="rcM4D">2.Softmax回归（多项/多类的Logistic回归）</h2>
Softmax回归也采用交叉熵作为损失函数。Softmax回归核心是应用Softmax函数将线性模型的输出转化为一个类别概率分布，其中Softmax函数会将每个类别计算得到的分数转换为概率

<h2 id="oLdq2">3.感知器</h2>
二分类：普通感知器--&gt;投票感知器--&gt;平均感知器

<p>多分类：广义感知器</p>
<p>逻辑：</p>
<p>感知器学习的参数和训练样本顺序相关：根据前面的样本学好了，预测很不错，结果在最后一个错了，更新参数，反而学的参数变差了。</p>
<p>解决这种问题需要提高感知器鲁棒性和泛化能力，给每个参数一个置信系数，最终分类结果由不同参数的感知器投票决定——投票感知器。</p>
<p>投票感知器对参数的保存记录有开销，因此通过参数平均策略减少参数数量——平均感知器</p>
<h2 id="BjJEa">4.支持向量机</h2>
【大部分都已经学习过了 不重复写入】

<h1 id="BSbRW">第4章 前馈神经网络</h1>
<h2 id="aY6wr">1.激活函数：</h2>
①Sigmoid型函数：(Hard)Logistic/(Hard)Tanh

<p>②ReLU函数：带泄露的/带参数的/ELU/Softplus</p>
<p>③Swish函数</p>
<p>④GELU函数</p>
<p>⑤Maxout单元</p>
<h2 id="B21rh">2.网络结构：前馈/记忆/图</h2>
<h2 id="ZHX5m">3.反向传播：</h2>
①前馈计算每层净输入和激活值，直到最后一层

<p>②反向传播计算每一层的误差项</p>
<p>③计算每一层参数的偏导数</p>
<h2 id="cCZ8z">4.自动梯度计算：数值微分/符号微分/自动微分</h2>
<h2 id="QBIO2">5.详细与补充</h2>
①误差项：目标损失函数关于某层的神经元（的净输入）的偏导数，用δ(l)来表示。

<p>②数值微分就是取很少的非零扰动Δx计算梯度，符号微分就是通过迭代或递归使用一些事先定义好的规则进行变换。</p>
<p>③自动微分可分为前向模式和反向模式，前向按照计算图中的计算方向递归计算梯度，反向按照相反方向、与反向传播计算梯度的方式相同。都是基于链式法则的。</p>
<h1 id="RvC5i">第5章 卷积神经网络</h1>
<h2 id="UqNkx">1.卷积咋算，一维卷积和二维卷积是啥，卷积核/滤波器是啥</h2>
①卷积核/滤波器就是信息的衰减率，每个时刻的信号乘上各自对应的衰减率，再加一起，就是卷积。标准来讲，信号序列和滤波器/卷积核的卷积定义为y=w*x，*就是卷积运算。

<p>②一维卷积就是信号序列是一行，二维卷积就是信号序列是一个二维表，二维卷积的算法就是从这个表里找到和卷积核表一样大小的一块区域，并对应卷积核表的相对位置，每个数都按位去乘起来，然后加起来得到这个数放到刚才说的对应位置里，其中在乘之前，需要把卷积核翻转一下。</p>
<p>③特征映射的概念：一幅图像卷积后得到的结果。</p>
<h2 id="uNV5G">2.卷积和互相关是啥关系</h2>
互相关和卷积的区别是卷积核是否翻转，互相关也可以称为不翻转卷积。

<h2 id="RQ4Kn">3.卷积的步长、零填充是干啥的</h2>
步长是卷积核滑动时的时间间隔，零填充是在输入向量两端进行补0，根据不同步长和填充有常用卷积：窄/宽/等宽卷积

<h2 id="qGazF">4.卷积的数学性质</h2>
卷积的数学性质是交换性和导数。

<p>其中导数说的是：卷积后通过激活函数，这个激活值关于输入序列的偏导，等于其对卷积的偏导和卷积核的卷积结果</p>
<h2 id="Zp6sp">5.卷积神经网络的构成，三个部分分别干啥的</h2>
卷积层：提取局部区域特征，不同卷积核是不同的特征提取器；

<p>汇聚层：选择特征、降低特征数量，从而减少参数数量。</p>
<p>汇聚层汇聚函数常用的有最大汇聚和平均汇聚，分别是取区域内神经元最大活性值和平均活性值。就是说，卷积层提取特征，汇聚层把特征压缩从而减少参数量。</p>
<p>【汇聚层=池化层; 最大/平均汇聚=最大/平均池化】</p>
<p>全连接层</p>
<h2 id="ywvHx">6.卷积网络的参数学习，也就是卷积网络的反向传播咋进行的，学习的参数是哪些</h2>
学习的参数是卷积核和偏置，汇聚层和卷积层反向传播的不同在于误差项的计算不同

<h2 id="LBQH1">7.典型的卷积神经网络</h2>
&gt; LeNet-5,AlexNet,Inception 网络,残差网络
&gt;

<p>①LeNet-5网络：值得注意的是用了连接表来定义输入和输出特征映射之间的依赖关系</p>
<p>【连接表：写出来关系，从而指导我们，让每一个输出特征都依赖于少数几个特征映射】</p>
<p>②AlexNet网络：将网络拆成两半分别放在俩GPU上</p>
<p>③Inception网络：特点是一个卷积层包含多个不同大小的卷积操作</p>
<p>④残差网络：思想是在用非线性单元逼近目标函数时拆分一下，h(x)=x+(h(x)-x)，前后两项分别是恒等函数和残差函数，让非线性单元近似残差函数简单点，然后+x就逼近了目标函数</p>
<h2 id="E2SWE">9.其他卷积方式：转置卷积、微步卷积、空洞卷积都分别是干啥的</h2>
①转置卷积：转置的是卷积核，作用是实现低维到高维的反向映射

<p>②微步卷积：想让步长小于1从而提高特征维数，但是步长没法小于1，所以在输入特征之间插入0间接减小步长</p>
<p>③空洞卷积：给卷积核加入一些空洞来增加卷积核的大小，从而能够不增加参数数量，同时增加输出单元感受野</p>
<p>【感受野：神经元只接受其所支配的刺激区域内的信号，只有这个区域内的刺激才能激活该神经元】</p>
<h1 id="KqUh4">第6章 循环神经网络</h1>
<h2 id="JiXEg">1.延时神经网络TDNN、有外部输入的非线性自回归模型NARX、循环神经网络RNN，三者分别是啥概念，之间有啥不同点和联系</h2>
<h3 id="TRMuo">三者的概念</h3>
**TDNN**给前馈网络非输出层加了延时器，延时器可以记录神经元最近几次活性值，l层活性值依赖于l-1层神经元最近K个时刻的活性值；

<p><strong>NARX</strong>也用延时器，延时器记录最近几次的外部输入和最近几次的输出，把这些输入、输出传入一个非线性函数，得到某时刻的输出；</p>
<p><strong>RNN</strong>用了带自反馈的神经元，将前一个活性值（由延迟器记录）和当前输入序列传入非线性函数，从而更新带反馈边的隐藏层的活性值（称为状态/隐状态）</p>
<h3 id="qioFq">三者的联系</h3>
**TDNN**仅依赖固定窗口的输入，不能捕捉长时间依赖，不使用反馈。

<p><strong>NARX</strong>显式依赖过去的输入和输出，有反馈机制，能处理短期依赖的输入-输出关系。</p>
<p><strong>RNN</strong>通过隐藏状态隐式记忆所有历史信息，能处理长时间依赖，但训练复杂。</p>
<p>TDNN适合处理短期依赖和固定窗口长度的任务，NARX适用于有明确输入输出关系，RNN适合处理复杂、长依赖关系的任务</p>
<h2 id="QdVQL">2.从简单循环网络SRN理解RNN的具体原理</h2>
SRN=两层前馈神经网络+隐藏层到隐藏层的反馈连接。隐藏层状态的计算见笔记图。

<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1730374448373-16f7a3ea-b431-41a5-b467-0c403b2d16bf.png"></p>
<h2 id="b0jK3">3.图灵完备是什么概念</h2>
图灵完备：可以实现图灵机的所有功能的数据操作规则称为图灵完备，如主流编程语言。

<p>p.s. 所有的图灵机都可以被一个由sigmoid型激活函数的神经元构成的全连接RNN来进行模拟</p>
<h2 id="NkCe6">4.RNN应用到不同类型机器学习任务对应的三种模式：序列到类别模式、同步的序列到序列模式、异步的序列到序列模式</h2>
①序列到类别模式：输入为序列，输出为类别。将整个序列的最终表示传入分类器进行分类。最终表示可以选最后时刻的状态，也可以选择整个序列所有的状态的平均（一个平均状态）

<p>②同步的序列到序列模式：每一时刻都有输入和输出，输入序列和输出序列长度相同。例如用于词性标注。</p>
<p>:::info<br>一个疑问：词性标注似乎只取决于当前词本身，而与上下文无关，为什么要采取RNN呢？</p>
<p>对应的解释：</p>
<p> a.多义词： 很多词具有多个词性，其确切词性往往取决于上下文。例如“银行”可以是名词（去银行办业务），也可以是动词（银行贷款）。             </p>
<p> b.同形异义词： 不同的词可能具有相同的形式，但词性不同。例如“打”可以是动词（打篮球），也可以是量词（一打啤酒）。                  </p>
<p> c.长距离依赖： 词性标注有时需要考虑较长的上下文信息。例如，一个代词的词性往往取决于它所指代的名词，而这个名词可能出现在较远的前面。</p>
<p>:::</p>
<p>③异步的序列到序列模式（编码器-解码器模型）：输入序列输出序列不一定严格对应，也不一定长度相等。编码器和解码器是两个不同的RNN，样本x按不同时刻输入编码器得到编码，再把编码传入解码器得到输出序列</p>
<h2 id="qynpL">5.RNN怎么进行参数学习—梯度下降—BPTT/RTRL</h2>
RNN的参数学习BPTT和RTRL见图。BPTT计算少，但要存所有时刻中间梯度，空间复杂度高；RTRL不需要梯度回传。

<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1730374448373-16f7a3ea-b431-41a5-b467-0c403b2d16bf.png"></p>
<h2 id="cVKXX">6.什么是梯度爆炸问题，什么是梯度消失问题，什么是长程依赖问题，怎么解决这些问题</h2>
①长程依赖问题：时刻t的输出依赖时刻k的输入，但t和k间隔很长，神经网络很难建模这种长距离依赖关系，称长程依赖问题。为什么难建模，是因为t和k间隔很长的情况下，引发梯度爆炸问题和梯度消失问题。【梯度爆炸问题、梯度消失问题见上图】

<p>②怎么解决：</p>
<p>梯度爆炸——权重衰减（正则化，限制参数取值范围使γ&lt;=1）/梯度截断（大于阈值就截断成较小的数）；</p>
<p>梯度消失——改变模型；为了解决俩问题，引入门控机制。</p>
<h2 id="vKftk">7.基于门控的RNN：什么是长短期记忆网络LSTM，什么是长短期记忆，LSTM的变体</h2>
①引入内部状态、候选状态、门控机制

<p>a. LSTM引入的内部状态：专门进行线性的循环信息传递，同时（非线性地）输出信息给隐藏层的外部状态；</p>
<p>b. LSTM引入的候选状态：从非线性函数输出得到。</p>
<pre><code>  c. 门控机制：输入门（控制当前时刻候选状态要保存多少信息）、遗忘门（控制上一时刻内部状态要遗忘多少信息）、输出门（控制当前时刻内部状态有多少要输出给外部状态）。三个门取值都在0，1之间表示以一定比例让信息通过。
</code></pre>
<p>②长期记忆：隐含从训练数据学到的经验，更新周期远远慢于短期记忆。长短期记忆的生命周期介于长期记忆和短期记忆之间。</p>
<p>③LSTM变体：</p>
<p>a.无遗忘门的；</p>
<p>b.三个门不但依赖于输入和上一时刻的隐状态，也依赖于上一个时刻的记忆单元的peephole连接；</p>
<p>c.耦合输入门和遗忘门（因为在LSTM中这两个门有一定的互补关系，所以合并成一个）</p>
<h2 id="fDylK">8.基于门控的RNN：门控循环单元网络GRU</h2>
GRU：不引入额外记忆单元，引入更新门——控制当前状态需要从历史状态中保留多少信息（不经过非线性变换，以及需要从候选状态中接受多少新信息。

<p>更新门=0时当前状态和前一时刻状态间为非线性函数关系；=1时为线性函数关系。引入重置门控制候选状态的计算是否依赖上一时刻状态。</p>
<p>重置门=0时候选状态只与当前输入有关，与历史无关；重置门=1时与当前输入和历史都有关，和简单RNN一致</p>
<h2 id="s8PiL">9.什么是深层RNN，怎么增加RNN的深度（堆叠RNN，双向RNN）</h2>
单看网络输入到输出间路径，RNN很浅。

<p>增加深度以增强RNN能力，方式主要是增加同一时刻网络输入到输出间路径。</p>
<p>①堆叠RNN：第l层网络输入是第l-1层网络输出，每层内部计算自己时时刻刻的隐状态。</p>
<p>②双向RNN：增加按时间逆序传递信息的网络层。有两层RNN构成，两个的输入相同，信息传递方向不同。</p>
<h2 id="UjjwK">10.RNN扩展到图结构：递归神经网络RecNN与图神经网络GNN</h2>
①RecNN：RNN在有向无循环图上的扩展

<p>②GNN：将消息传递的思想扩展到图数据结构上，每个节点可以接收相邻节点消息并更新自己状态</p>
<p>【消息传递思想：隐状态看作节点，节点收到父节点的消息，更新自己状态，传递给子节点】</p>
<h1 id="qUcQA">第7章 网络优化与网络正则化</h1>
<h2 id="gHwZF">一、网络优化问题</h2>
&gt; 网络优化问题——找到更好的局部最小值（平坦的）和提高优化效率
&gt;

<h3 id="ihLIC">1.从算法的优化出发，用更有效的优化算法：着眼于批量大小 / 学习率 / 梯度估计</h3>
<h4 id="A7Rga">（1）批量大小的选择：</h4>
批量小：越有可能收敛到平坦最小值，，但需要设置较小学习率，否则模型会不收敛

<p>批量大：训练稳定，可设置较大学习率，但越有可能收敛到尖锐最小值</p>
<p>批量大小较小时，可采用<u>线性缩放准则</u>【批量大小增加m倍时学习率也增加m倍】</p>
<p>=&gt;适当小批量</p>
<h4 id="ITMan">（2）学习率的调整：</h4>
<h5 id="Dq3D9">①学习率衰减（学习率退火）</h5>
概念：经验上，学习率开始时保持大些以保证收敛速度，收敛到最优附近时小些以避免振荡

<p>方法：分段常数衰减（阶梯衰减）/ 逆时衰减 / 指数衰减 / 自然指数衰减 / 余弦衰减 </p>
<h5 id="EDUhS">②学习率预热：</h5>
Why：批量大时，需要较大学习率，但开始时梯度也大，初始学习率也大的话，训练不稳定

<p>概念：开始时用小学习率，梯度下降到一定程度后再恢复初始学习率</p>
<p>方法：逐渐预热【按迭代次数从比较小的学习率逐渐增大，直至回到初始学习率】</p>
<h5 id="cteg7">③周期性学习率调整：</h5>
Why：参数在尖锐最小值，增大学习率有助于逃离；在平坦最小值，增大后依然可能在其吸引域内

<p>概念：周期性增大学习率</p>
<p>方法：循环学习率【让学习率在一个区间内周期性增大/缩小】【if线性缩放=&gt;三角循环学习率】</p>
<p>  带热重启的随机梯度下降【每间隔一定周期，学习率重新初始化为某预先设定值，然后衰减】</p>
<h5 id="Gdm6L">④AdaGrad算法：</h5>
概念：每次迭代时先计算每个参数梯度平方累计值，效果上累计大的学习率小，累计小的学习率大

<p>【但整体学习率随迭代次数增加而缩小】</p>
<p>缺点：if迭代多次还没找到最优点，那基本没希望了，因为学习率已经被搞得很低了</p>
<h5 id="BCPfw">⑤RMSprop算法：</h5>
与AdaGrad区别：不算累计，算每次迭代的梯度平方的指数衰减移动平均

<p>优点：每次迭代，每个参数学习率并不一定衰减，可能小可能大。</p>
<h5 id="Jh4vg">⑥AdaDelta算法：</h5>
与AdaGrad和RMSprop区别：不仅用梯度平方的指数衰减移动平均来调整学习率，还引入每次参数更新差值的平方的指数衰减权移动平均

<p>优点：在一定程度上平抑了学习率的波动</p>
<p>  【将RMSprop中的初始学习率改为动态计算的参数更新插值的平方的指数衰减权移动平均】</p>
<h4 id="DefH7">（3）梯度估计修正【cuz每次迭代的梯度估计和整个训练集上的最优梯度并不一致，具有随机性】</h4>
<h5 id="ely6I">①动量法：</h5>
概念：计算负梯度的“加权移动平均”作为参数的更新方向；相当于模拟物理中动量的概念

<p>效果：参数实际更新差值取决于最近一段时间内梯度的加权平均值</p>
<p>  【方向一致，参数更新幅度大，加速；方向不一致，梯度更新幅度小，减速】</p>
<h5 id="HoJsK">②Nesterov加速梯度（Nesterov动量法）</h5>
概念：对动量法的改进。改进在“负梯度”是对谁的梯度。

<p>改进：动量法分两步：</p>
<p>根据上一步参数更新方向更新一次得到参数theta hat；</p>
<p>再用当前梯度的反方向进行更新。</p>
<p>   问题在于当前梯度说的还是上一时刻的梯度，而不是theta hat的梯度，改为后者更合理。</p>
<h5 id="VfyZd">③Adam算法：</h5>
概念：可看作动量法和RMSprop的结合，使用动量作为参数更新方向，且自适应调整学习率

<p>计算：梯度平方的指数加权平均【类似RMSprop】+梯度的指数加权平均【类似动量法】</p>
<pre><code>    分别看作：梯度均值 和 未减去均值的方差。迭代初期值比真实小，偏差大，因此需做修正
</code></pre>
<p>进一步的改进：引入Nesterov加速梯度，称为Nadam算法</p>
<h5 id="SpUCz">④梯度截断：</h5>
概念：梯度爆炸时，梯度的模大于一定阈值，对梯度进行截断

<p>方法：按值截断【超过区间，梯度就设为区间上限；低于区间，梯度就设为区间下限】</p>
<p>  按模截断【截断到一个给定的截断阈值b】</p>
<h3 id="C1YZT">2.从参数初始化的角度出发，用更好的参数初始化方法：</h3>
<h4 id="zvFkm">（1）预训练初始化：</h4>
①概念：用已经在大规模数据上训练过的模型提供的参数初始值

<p>②优点：通常具有更好的收敛性和泛化性</p>
<p>③缺点：灵活性不够，不能任意调整网络结构</p>
<h4 id="dAmdp">（2）固定值初始化：</h4>
概念：对特殊参数，根据经验，用特殊固定值，初始化

<h4 id="T4OKj">（3）随机初始化：</h4>
①概念：避免全初始化为0导致的对称权重现象，对每个参数随机初始化，提高不同神经元间区分性

<p>②基于固定方差的参数初始化：</p>
<p>概念：从固定均值&amp;方差的分布中采样，生成参数初始值</p>
<p>方法：高斯分布初始化【用高斯分布𝒩(0, 𝜎2)】</p>
<pre><code>均匀分布初始化【[−𝑟, 𝑟] 内采用均匀分布】
</code></pre>
<p>问题：参数范围太小【导致神经元输出过小、Sigmoid型激活函数丢失非线性能力】</p>
<pre><code>       	参数范围太大【导致输入状态过大、Sigmoid激活值饱和，梯度消失】
</code></pre>
<p>解决：配合逐层归一化</p>
<h4 id="iJYyH">③基于方差缩放的参数初始化：</h4>
概念：根据神经元的连接数量自适应调整初始化分布的方差

<p>  【输入连接多，每个输入连接上的权重就应该小些，避免输出过大或过饱和/// vise versa】</p>
<p>方法：Xavier初始化【根据 𝑀𝑙−1 + 𝑀𝑙（分别是𝑙−1和𝑙 层神经元数量）初始化参数𝑙 层方差】</p>
<p>  【适用激活函数：恒等函数、Logistic函数、Tanh函数】</p>
<pre><code>      He初始化【使用ReLU函数时通常一半神经元输出为0，分布的方差近似为恒等函数一半】
</code></pre>
<h4 id="JRwo3">④正交初始化：</h4>
概念：将某层的权重矩阵初始化为正交矩阵

<p>原因：用正交矩阵的性质，保证传播过程的范数保持性</p>
<p>步骤：用均值为0方差1的高斯分布初始化一个矩阵</p>
<pre><code>by奇异值分解得两个正交矩阵，用其一作为权重矩阵
</code></pre>
<h3 id="s2SRg">3.数据预处理</h3>
<h4 id="sjoFQ">（1）尺度不变性：算法缩放全部/部分特征后不影响学习和预测</h4>
      尺度敏感：影响

<p>  =&gt;对样本预处理，各维特征转换到相同取值区间，消除不同特征相关性</p>
<h4 id="M7Oj4">（2）归一化：把数据特征转换为相同尺度</h4>
方法：最小最大值归一化 / 标准化（Z值归一化）/ 白化

<h3 id="SFViG">4.逐层归一化</h3>
（1）概念：数据归一化应用到神经网络中隐藏层的输入中

<p>（2）Why：</p>
<p>①更好的尺度不变性：低层参数改变不影响高层输入保持稳定+更高效进行参数初始化和超参选择</p>
<p>②更平滑的优化地形：优化地形平滑，梯度稳定，允许更大学习率，提高收敛速度</p>
<p>（3）常用方法：批量归一化 / 层归一化 / 权重归一化 / 局部响应归一化</p>
<h3 id="whF4i">5.超参数优化</h3>
<h4 id="AjPEd">（1）超参数</h4>
①网络结构【神经元间连接关系、层数、每层神经元数量、激活函数类型】

<p>②优化参数【优化方法、学习率、小批量样本数量】</p>
<p>③正则化系数</p>
<h4 id="G9eoa">（2）简单方法</h4>
①网格搜索：尝试所有超参组合，分别训练模型，测试性能，选取最好的配置

<p>②随即搜索：超参随机组合，然后思路同上</p>
<p>③贝叶斯优化：根据当前已实验的超参组合，预测下个可能最好的组合</p>
<p>Eg. 时序模型优化（SMBO）</p>
<p>需定义收益函数，Eg. 期望改善函数（EI函数）/ 改善概率 / 高斯过程置信上界（GP-UCB）</p>
<p>④动态资源分配：</p>
<p>思想：预先估计配置效果，差就中止评估，转而将资源留给其他配置</p>
<p>常用策略：早期停止策略</p>
<p>一种有效方法：逐次减半【但需要注意 超参数配置的数量N 的设置】</p>
<p>⑤神经架构搜索：通过神经网络来自动实现网络架构的设计</p>
<h2 id="micrX">二、网络正则化问题</h2>
:::info
"""

<p>在传统机器学习中常采用ℓ1 和 ℓ2 正则化来限制模型复杂度，避免过拟合；</p>
<p>但在训练深度神经网络，特别是过度参数化（模型参数数量远大于训练数据数量）时，用其他方法更好。</p>
<p>“””</p>
<p>:::</p>
<h3 id="d2Nsu">1.ℓ1 和 ℓ2 正则化</h3>
&gt; 就是在参数优化时加了一个λℓ(θ)，其中ℓ可以是一阶范数也可以是二阶范数，λ为正则化系数
&gt;

<p>（1）注意点1：ℓ1范数在零点不可导，需要用其他方式近似</p>
<p>（2）注意点2：ℓ1范数约束通常使优化出来的最优参数位于坐标轴上，使得最终参数为稀疏性向量（有好多0）</p>
<p>（3）注意点3：同时加入ℓ1和ℓ2正则化（相应的用两个正则化系数）称为弹性网络正则化</p>
<h3 id="e4xZY">2.权重衰减</h3>
每次参数更新时引入衰减系数β，使得θt=(1-β)θt-1-αgt，gt是更新时梯度，α是学习率

<h3 id="pVFJj">3.提前停止</h3>
整一个验证集，验证集错误率不下降，就停止迭代优化，不继续下去了

<h3 id="OwsR9">4.丢弃法</h3>
随机丢弃神经元及其连接来避免过拟合，用掩蔽函数mask(·)来做这件事

<p>（1）掩蔽函数做了啥：对于某层神经元的输入，训练阶段用丢弃掩码m来丢弃一些输入从而丢弃掉神经元及其连接（m由概率为p的伯努利分布随机生成）；测试阶段同样用概率p来乘所有输入来平衡被丢掉的和留下的输入</p>
<p>（2）集成学习角度的解释：每次迭代相当于训练一个不同的、共享原始网络参数的子网络；最终网络是子网络的集成</p>
<p>（3）贝叶斯学习角度的解释：（先验分布*网络）在所有参数上的积分，可以用M次dropout后的网络的算数平均来近似</p>
<p>（4）RNN上的dropout：</p>
<pre><code>    ①对非时间维度的连接（非循环连接）进行随即丢失【时间维度记录了隐状态不能乱丢】

    ②变分丢弃法：对参数矩阵的每个元素进行随机丢弃，并且在所有时刻都要使用相同的丢弃掩码
</code></pre>
<h3 id="sOpKA">5.数据增强</h3>
通过算法对数据转变来增加数据量，避免在小数据量数据集上过拟合了，方法有旋转/翻转/缩放/平移/加噪声等

<h3 id="naOcJ">6.标签平滑</h3>
在输出标签中添加噪声，比如一些错误标注了的样本，最小化这些样本上的损失函数就会导致过拟合，所以加噪

<p>（1）注意点1：标签用one-hot表示可看作硬目标，softmax分类器+min交叉熵损失函数会导致正确类和其他类权重差异很大，导致过拟合，所以对标签的one-hot编码来加噪，变成软目标，避免模型输出过拟合到硬目标上</p>
<p>（2）注意点2：按照类别相关性来给不同标签不同的噪声概率，称为知识蒸馏</p>
<h1 id="pqnN3">第8章 注意力机制与外部记忆</h1>
<h2 id="SIeD9">1.什么是注意力</h2>
（1）概念：有意或无意从大量输入中选择小部分有用信息来重点处理，并忽略其他信息的能力

<p>（2）分类：聚焦式注意力/选择性注意力【自上而下的有意识的】、基于显著性的注意力【自下而上的无意识的】</p>
<h2 id="hCqJk">2.注意力机制</h2>
&gt; 作为一种资源分配方案，有限的计算资源用来处理更重要的信息
&gt;

<h3 id="J42Hu">（1）计算方式</h3>
:::info
在所有输入信息上计算注意力分布，然后根据注意力分布来计算输入信息的加权平均

<p>:::</p>
<pre><code>  ①注意力分布：给定查询向量和输入向量集合，选择第n个输入向量的概率分布

  ②加权平均：计算（选择第n个输入向量的概率 * 第n个输入向量）的累加
</code></pre>
<h3 id="U08c6">（2）注意力机制变体</h3>
      ①硬性注意力：只关注某个输入向量，要么选取最高概率的输入向量，要么在注意力分布式上随机采样

<p>:::info<br>无法用反向传播训练了，需要使用强化学习，所以一般用软性注意力【软性注意力就是（1）中的那种注意力】</p>
<p>:::</p>
<pre><code>  ②键值对注意力：用键值对表示输入，键用来计算注意力分布，值用来计算聚合信息。这种方式的特殊点是输入信息的形式

  ③多头注意力：用多个查询向量，并行地从输入中选多组，这样使得每个注意力关注输入信息的不同部分

  ④结构化注意力：假设输入信息的重要程度存在差异，从而形成了层次结构

  ⑤指针网络：仅采用注意力机制的注意力分布作为软性指针，指出相关信息的位置，输出序列是输入序列的下标
</code></pre>
<h2 id="qolOn">3.自注意力（内部注意力）模型</h2>
&gt; 为了建立输入序列之间的长距离依赖关系而使用
&gt;

<h3 id="OtgCs">（1）思想</h3>
用注意力机制来“动态”生成不同连接的权重的全连接网络，注意动态生成的是不用连接的权重

<h3 id="fPfC0">（2）做法</h3>
采用QKV（查询-键-值）模式。先把每个输入映射到三个不同空间，分别得到了查询向量、键向量和值向量。然后用键值对注意力机制的原理（2.（2）.②），得到输出向量。

<h3 id="rA0a2">（3）用法</h3>
替换卷积层和循环层，也可以和这俩一起交替使用。

<h3 id="L8j8H">（4）缺点</h3>
忽略了输入信息的位置信息，单独使用时需要加入位置编码信息来进行修正

<h2 id="ZbcYs">4.“记忆”的了解与认识</h2>
长期记忆与短期记忆与工作记忆

<p>:::info<br>长期记忆类比人工神经网络中的权重参数，短期记忆类比人工神经网络中的隐状态</p>
<p>:::</p>
<p>记忆在大脑皮层是分布式存储而不是局部存储</p>
<p>联想记忆是基于内容寻址的存储</p>
<p>工作记忆是临时存放和某任务相关的短期记忆和其他相关的内在记忆；临时存储和处理系统；维持时间短；”缓存“；容量小</p>
<p>外部记忆是神经网络借鉴工作记忆原理引入的</p>
<h2 id="FFknm">5.记忆增强神经网络MANN</h2>
:::info
**记忆增强神经网络MANN（记忆网络MN）：装备外部记忆的神经网络**

<p>:::</p>
<h3 id="zEOcS">（1）端到端记忆网络MemN2N</h3>
要存储的信息转换成两组记忆分别用于寻址和输出，然后主网络根据输入生成查询向量，用键值对注意力机制来读取记忆并产生输出。

<p>【特点：外部记忆单元只读】</p>
<h3 id="layUo">（2）神经图灵机NTM</h3>
控制器接收（当前输入+上时刻输出+上时刻读取外部记忆的信息），然后产生（当前输出+查询向量+删除向量+增加向量），然后开始读写。

<p>读，从外部记忆读取信息，采用注意力机制来进行基于内容的寻址；</p>
<p>写，根据注意力分布按比例删除和增加每个记忆片段的删除向量和增加向量。</p>
<p>【特点：外部记忆可读写】</p>
<p>:::info<br>多跳操作：主网络根据上一次从外部记忆读取的信息，生成一个新的查询向量，而这个查询向量又继续用于读取外部记忆，这样循环多轮进行交互，从而可以实现更复杂的计算</p>
<p>:::</p>
<h2 id="v62sm">6.基于神经动力学的联想记忆</h2>
:::color2
是基于**内容**寻址的

<p>应用：原理所对应的模型 or 用来增加外部记忆</p>
<p>两种联想记忆模型：</p>
<p>自联想模型：输入的模式与输出的模式在同一空间</p>
<pre><code>      		  eg.可通过前馈/循环神经网络等实现
</code></pre>
<p>异联想模型：输入的模式和输出的模式不在同一空间</p>
<pre><code>         	  eg.大多机器学习问题可看作异联想，可作分类器
</code></pre>
<p>:::</p>
<h3 id="kprvT">（1）Hopfield网络</h3>
:::info
Hopfield网络是由一组互相连接的神经元组成的RNN。

<p>【属于自联想模型；每个神经元既输入也输出，无隐藏神经元】</p>
<p>:::</p>
<p>①更新方式</p>
<p>异步更新【每次更新一个神经元，顺序可随机也可预先指定】</p>
<p>同步更新【一次性更新所有神经元，需要时钟】</p>
<p>②能量=每个不同的网络状态【是标量】；</p>
<p>   网络收到外部输入后会演化到某个稳定状态</p>
<p>③稳定状态=吸引点Attractor=能量局部最优点=能量函数的局部最小点=网络中存储的模式Pattern</p>
<p>④检索：从接收到网络输入，随时间收敛到吸引点上的过程。</p>
<pre><code> 【每个吸引点对应一个管辖区域，输入落到该区域会收敛到该点】
</code></pre>
<p>⑤存储容量怎么提升：改进网络结构、学习方式、引入更复杂的运算</p>
<h3 id="V6vM8">（2）使用联想记忆增加网络容量</h3>
①将联想记忆模型作为部件引入LSTM网络

<p>②将RNN的部分连接权重作为短期记忆，并通过一个联想记忆模型继续更新</p>
<h1 id="NZJpq">第9章 无监督学习</h1>
<h2 id="Hz2Zu">1.无监督学习问题分类</h2>
<h3 id="Im47V">（1）无监督特征学习</h3>
从无标签训练数据中挖掘有效特征/表示

<p>用来降维、数据可视化、监督学习前期的数据预处理</p>
<h3 id="P1szp">（2）概率密度估计</h3>
根据一组训练样本来估计样本空间的概率密度

<p>根据<strong>假设/不假设</strong>数据服从某个<strong>已知概率密度函数形式的分布</strong>，可以分为参数密度估计、非参数密度估计</p>
<h3 id="SqgRs">（3）聚类</h3>
将一组样本根据一定的准则划分到不同的组（即簇）

<p>eg.谱聚类、K-Means</p>
<h2 id="DN58a">2.无监督特征学习</h2>
<h3 id="giWWs">（1）主成分分析PCA——数据降维方法</h3>
①思想：使得在转换后的空间中数据的方差最大，从而最大化数据差异性，保留更多原始数据信息

<p>②手段：选择数据方差最大方向进行投影</p>
<p>③用处：作为监督学习的数据预处理方法，去除噪声并减少特征间相关性等</p>
<p>④局限性：不能保证投影后数据类别可分性更好</p>
<p><strong>PCA更详细内容见下：</strong></p>
<p><strong>李航《机器学习方法》中关于PCA的部分：</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.yuque.com/yuqueyonghucoit3e/wefx9h/mchuht0itomgngxq">第十六章 主成分分析</a></p>
<p><strong>周志华《机器学习》中关于PCA的部分：</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.yuque.com/yuqueyonghucoit3e/wefx9h/yg0rqrvmr4a9506x">主成分分析</a></p>
<h3 id="I0mF6">（2）稀疏编码</h3>
①编码是啥：对 𝐷 维空间中的样本 𝒙 找到其在 𝑃 维空间中的表示（或投影）

<p>②怎么得到编码：得找到一组完备的基向量（例如主成分分析PCA找到的主成分/投影矩阵）</p>
<p>③啥是完备：M个基向量能支撑M维的欧氏空间，就是完备，能支撑比M还高维的，就是过完备</p>
<p>④为啥要稀疏：稀疏好啊，稀疏有优点：</p>
<p>a. 计算量上：极大地降低计算量</p>
<p>b. 可解释性上：将输入样本表示为少数几个相关特征，更好描述特征</p>
<p>c. 特征选择上：可以实现特征自动选择，自动是指只选择和输入样本最相                                关的少数特征，能降低噪声，减轻过拟合</p>
<p>⑤怎么得到稀疏编码：得到编码，需要完备基向量，那稀疏编码，就需要过完备的基向量</p>
<p>⑥那怎么得到过完备基向量：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1730552224573-82cca291-9947-4ea8-961c-86189850d4b2.png"></p>
<p>按照这个图来找到，其中：</p>
<p>𝒁 = [𝒛<sup>(1)</sup>, ⋯ , 𝒛<sup>(𝑁)</sup>]， 这个集合里面每个元素都是一组基向量的系数𝒛，</p>
<pre><code>𝒛 = [𝑧1, ⋯ , 𝑧𝑀] ，𝒛 也称为一个输入样本 𝒙 的编码
</code></pre>
<p>【1个样本，对应1组基向量，一个系数集合𝒛】</p>
<p>𝜌(⋅) 是一个稀疏性衡量函数，𝒛 越稀疏，𝜌(𝒛) 越小，函数可以选择</p>
<pre><code>                          ℓ1 范数 / 对数函数 / 指数函数
</code></pre>
<p>𝜂 是一个超参数，用来控制稀疏性的强度</p>
<h3 id="pwkxW">（3）自编码器</h3>
①干啥的：（D维数据+自编码器）——映射—— 得到M维特征空间下，每个样本的编码

<p><strong><font style="color:#DF2A3F;">而这个编码，可以重构出原来的样本</font></strong></p>
<p><strong><font style="color:#DF2A3F;">自编码器可以得到有效的数据表示，例如最小重构错误、稀疏性等</font></strong></p>
<p>②结构：编码器 f（把数据从D维映射到M维）</p>
<p>解码器 g（把数据从M维重构到D维）</p>
<p>③学习目标：既然是为了要重构原来的样本，就应该<strong>最小化重构错误</strong></p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1731157201110-cbd55a19-fec0-43f8-a017-9d0bae32bb06.png"></p>
<p>也就是最小化上面这个式子</p>
<p>④根据维度来讨论：</p>
<p>a. 𝑀 ≥ 𝐷：一定可以找到一组或多组解使得 𝑓 ∘ 𝑔 为单位函数并使得重构错误为 0</p>
<p>然后<strong>加一些约束</strong>，比如要求编码只能取K个不同值，那就转换为K类的聚类问题的解了</p>
<p>b. 𝑀 &lt; 𝐷：相当于一种降维/特征抽取方法了</p>
<p>⑤简单自编码器：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1731157508881-3602b56b-1def-49ba-ad3f-213a43d178aa.png"></p>
<p>如果W2=W1的转置，就成为捆绑权重，这会使得自编码器参数少，更容易学习，还起到一定的正则化作用。</p>
<p>通过最小化重构错误 <code> ℒ=  ∑  ‖𝒙(𝑛) − 𝒙′(𝑛))‖2 + 𝜆‖𝑾 ‖2𝐹 .</code> 学习网络参数</p>
<h3 id="XQHlz">（4）稀疏自编码器</h3>
①概念：𝑀 &gt; 𝐷，且让我们映射到的这个M维特征空间下，样本编码尽量稀疏

<p>②学习目标：最小化重构错误，目标函数为<code>ℒ=  ∑  ‖𝒙(𝑛) − 𝒙′(𝑛))‖&lt;sup&gt;2&lt;/sup&gt; + 𝜂𝜌(𝒁) + 𝜆‖𝑾 ‖&lt;sup&gt;2&lt;/sup&gt;</code></p>
<p>其中稀疏性度量函数 𝜌(𝒁)可以有多种定义：</p>
<p>a. 利用“稀疏编码”一节中的公式，分别计算每个编码稀疏度，再求和</p>
<p>b. 定义为一组训练样本中，<strong>每一个神经元激活的概率</strong></p>
<pre><code>这个概率可用隐藏层某个神经元的**平均活性值**来近似

希望该平均活性值接近于事先给定的某值，用KL距离来衡量差异
</code></pre>
<p>   <code>KL(𝜌∗|| ̂ 𝜌𝑗) = 𝜌∗ log 𝜌∗ ̂ 𝜌𝑗 + (1 − 𝜌∗) log [(1 − 𝜌∗) / (1− ̂ 𝜌𝑗)] </code></p>
<pre><code>所以也就是将稀疏性度量函数定义为 𝜌(𝒁) =  ∑ KL(𝜌∗|| ̂ 𝜌𝑗).
</code></pre>
<h3 id="rR8RA">（5）堆叠自编码器</h3>
用逐层堆叠方式来训练一个深层的自编码器，采用逐层训练来学习网络参数

<h3 id="YAOig">（6）降噪自编码器</h3>
①目的是干啥的：对数据部分损坏的鲁棒性——希望从坏了的数据中也能得到有效数据表示，恢复完整原始信息

<p>②怎么达到的目的：引入噪声增加编码鲁棒性</p>
<p>③过程：向量x—按比例μ随机将部分维度值设为0—破损向量x—输入给自编码器—编码z—重构出无损原始输入x</p>
<h2 id="CV45h">3.概率密度估计</h2>
<h3 id="DRqd4">（1）参数密度估计</h3>
①概念：根据先验知识，假设随机变量服从某种分布，然后通过训练样本，估计分布的参数

<p>②过程：第一，假设服从<code>𝑝(𝒙;𝜃)</code>这样一个概率分布函数</p>
<p>第二，求对数似然函数，也就是求个对数，<code>log 𝑝(𝒟;𝜃) = ∑ log 𝑝(𝒙(𝑛);𝜃)</code></p>
<p>第三，最大似然估计MLE，寻找使<code>log 𝑝(𝒟;𝜃)</code>最大的<code>𝜃</code>，使参数估计问题变成了最优化问题</p>
<p>③会遇到的问题：</p>
<p>a. 这个概率密度函数的选取，可能不那么好选，因为实际数据分布往往复杂，不是简单的分布</p>
<p>b. 训练样本只包含部分可观测变量，有些关键变量无法观测到，就很难准确估计真实分布</p>
<p>c. 维数灾难，高维数据维数越高，参数估计问题所需要的样本就越多，样本不足时会过拟合</p>
<p>④常见的概率分布函数的选取：</p>
<p>a. 正态分布：要估计的参数是均值<code>μ</code>和方差<code>Σ</code></p>
<p>b. 多项分布：要估计的参数是第k个状态的概率<code>μ&lt;sub&gt;k&lt;/sub&gt;</code>和拉格朗日乘子<code>λ</code></p>
<pre><code>                   （引入拉格朗日乘子将问题转换成了无约束优化问题）
</code></pre>
<h3 id="LL2kC"> （2）非参数密度估计</h3>
<h4 id="VXEMr">①概念</h4>
不假设数据服从某种分布了，而是把样本空间划分成不同区域，估计每个区域的概率，近似数据的概率密度函数`𝑝(𝒙)`

<p>每个区域的概率：<code>𝑃&lt;sub&gt;𝐾&lt;/sub&gt; = C&lt;sup&gt;𝑁&lt;/sup&gt;&lt;sub&gt;𝐾&lt;/sub&gt; 𝑃&lt;sub&gt;𝐾&lt;/sub&gt; (1 − 𝑃)&lt;sup&gt;1−𝐾&lt;/sup&gt;</code><sup> </sup>【二项分布】</p>
<p>N非常大时，近似：<code>𝑃≈ 𝐾/𝑁</code></p>
<p>又假设区域足够小（V足够小），则内部概率密度均匀、相同：<code>𝑃 ≈ 𝑝(𝒙)𝑉</code></p>
<p>然后我们就得到了<code>𝑝(𝒙)≈ 𝐾/𝑁𝑉</code></p>
<h4 id="y6PeY">②总结一下条件要求</h4>
**N足够大+V足够小**

<p>但V过小：落入区域样本少，概率密度不准确</p>
<p>解决方案：</p>
<p>A. 固定区域大小V，统计落入不同区域的数量——直方图方法、核方法</p>
<p>B. 改变区域大小V，使得落入每个区域的样本数量都为K个——K近邻方法</p>
<h4 id="kNgEq">③直方图方法</h4>
![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1731241433309-cceb05f3-fbce-4f8a-934f-2a1e30392b6f.png)

<p>优点：处理低维变量快速可视化数据分布</p>
<p>缺点：很难扩展到高维变量</p>
<p>   需要的样本数量随维度增加而指数增长，导致维数灾难</p>
<h4 id="VXNh3">④核方法（核密度估计，也叫 Parzen 窗方法）</h4>
原理就是定义核函数，如果落在核函数表示的空间内，核函数的值为1，否则为0，

<p>从而每一个点的密度估计就可以用：</p>
<p>（落入区域的样本数量K、）训练样本数量N、核函数表示空间的边长H来得到</p>
<h5 id="LqM7K">超立方体核函数</h5>
![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1731242339850-1d873b55-62b1-43e1-8646-fa8e6eab03db.png)![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1731242356384-deb08fe7-6b60-4dfc-8d9d-f4defdcc8c54.png)

<p>然后就得到了：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1731242371652-7d5125da-6980-477d-b31f-0791c7085cd9.png"></p>
<h5 id="UbbNz">高斯核函数</h5>
![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1731242424604-234eca7b-08a3-43be-8cc8-00001d5f8724.png)

<p>然后就得到了：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1731242448718-57d4ee0b-e42e-49e7-ab00-049fce19a979.png"></p>
<h4 id="IX3Od">⑤K近邻方法（KNN）</h4>
KNN用于分类已经很熟悉了

<p>在密度估计上，KNN是这么做的：</p>
<p>设置可变宽度区域，使落入每个区域样本数量都是为固定数量K个</p>
<p>然后我们在估计某点的密度时，找到以这个点为中心的球体，这个球体的大小使得落入球体的样本数量也是K个。</p>
<p>从而我们得到了V，而N是训练样本数量已知，K我们预先设定。</p>
<p>然后根据<code>𝑝(𝒙)≈ 𝐾/𝑁𝑉</code>就可以了。</p>
<h1 id="vnKAo">第10章 模型独立的学习方式</h1>
&gt; 模型独立：学习方式不限于具体模型（但是也会因模型特性不同而有不同程度的效果）
&gt;

<h2 id="czZNW">一、集成学习</h2>
<h3 id="Vhx1E">1.思想</h3>
![](https://cdn.nlark.com/yuque/0/2024/jpeg/40781129/1731583480002-1a07602d-01de-4c33-8658-5f05c9a74693.jpeg)

<p>“三个臭皮匠赛过诸葛亮”的同时，需要每个基模型差异尽可能大（和而不同）</p>
<p><strong><u>=&gt; 使差异大的方式：Bagging类方法，Boosting类方法</u></strong></p>
<h3 id="yHj3Y">2.Bagging类方法</h3>
<h4 id="lpPJa">（1）思想</h4>
引入随机性：随机构造训练样本/随机选择特征

<h4 id="jFr9w">（2）Bagging——随机构造训练样本</h4>
在原始数据集上进行有放回的随机采样，得到M个小训练集，训练M个模型，然后投票

<p><img src="https://cdn.nlark.com/yuque/0/2024/jpeg/40781129/1731583942158-8d1d144b-df6a-4211-9043-165de7c02a15.jpeg"></p>
<h4 id="QR2dy">（3）随机森林——Bagging+随机选择特征</h4>
每个基模型都是一颗决策树

<h3 id="gYTH4">3.Boosting类方法</h3>
<h4 id="NXXKT">（1）思想</h4>
每个基模型针对**前序模型的错误**进行专门训练

<p> =&gt;前序模型发生错误的训练样本的权重，调高！反之，调低！</p>
<h4 id="r6F5d">（2）目标</h4>
学习一个加性模型：`加性模型 = 从1到M求和（第m个基分类器的集成权重 * 第m个基分类器）`

<pre><code>                                          （基分类器=弱分类器）
</code></pre>
<h4 id="DJ4Ax">（3）AdaBoost算法</h4>
<h5 id="Ktc2S">①思想</h5>
通过**改变数据分布**来提高分类器的差异，迭代训练

<p>=&gt;每轮训练增加分错样本的权重，减少分对样本的权重</p>
<h5 id="nhC6N">②算法</h5>
        * 根据第`m`轮的样本权重，学习第`m`个分类器`𝑓<sub>𝑚</sub>`
        * 计算`𝜖<sub>𝑚</sub>`，`𝜖<sub>𝑚</sub>`是弱分类器`𝑓<sub>𝑚</sub>`在数据集上的加权错误
        * 计算`𝛼<sub>𝑚</sub>`，`𝛼<sub>𝑚</sub>`是第`𝑚`个分类器在集成时的权重，`𝛼<sub>𝑚 </sub>← 1/2 * log (1 −𝜖<sub>𝑚</sub>/𝜖<sub>𝑚</sub>)`
        * 调整样本权重，`𝑤<sup>(𝑛) </sup><sub>𝑚+1</sub> ← 𝑤<sup>(𝑛)</sup> <sub>𝑚</sub> exp ( − 𝛼<sub>𝑚</sub>𝑦<sup>(𝑛)</sup>𝑓<sub>𝑚</sub>(𝒙<sup>(𝑛)</sup>)), ∀𝑛 ∈ [1, 𝑁]`
        * 根据第`m+1`轮的样本权重，学习第`m+1`个分类器`𝑓<sub>𝑚+1</sub>`
        * 。。。。。。。

<h4 id="a9Te4">（4）《机器学习方法》中关于Boosting的内容</h4>
[第8章 Boosting](https://www.yuque.com/yuqueyonghucoit3e/wefx9h/lu10ii76mdplhi4g)

<h2 id="qEacI">二、自训练和协同训练——半监督学习算法</h2>
<h3 id="sY5OH">0.半监督学习</h3>
少量标注数据 + 大量无标注数据 --&gt; 进行学习

<h3 id="zuref">1.自训练/自举法</h3>
<h4 id="SxkPp">（1）思想</h4>
有标注的数据——训练模型——模型预测无标住样本标签——预测置信度高的样本+模型预测的无标注样本的标签

<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1731587795982-6676d8ef-9874-4c8c-a522-cefc1bb7563b.png"></p>
<h4 id="A4Ege">（2）缺点</h4>
无法保证伪标签是正确的——if伪标签是错的，反而会损害模型预测能力

<h3 id="iJ5rn">2.协同训练——自训练的一种改进方法</h3>
:::info
**视角**的概念：  
    数据的不同侧面

<p>eg.每个网页的类别，可以从文字内容判断（视角V<sub>1</sub>），也可以根据网页间链接关系判断（视角V<sub>2</sub>）</p>
<p>:::</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1731589553571-29e6de06-557f-4ae4-8f99-99be0e938f51.png"></p>
<p>训练出的模型f1，f2具备两个不同的视角，具有一定的互补性</p>
<h2 id="m0RWe">三、多任务学习</h2>
<h3 id="LOdVw">1.思想</h3>
**<font style="color:#DF2A3F;">同时</font>**学习多个**<font style="color:#DF2A3F;">相关任务</font>**，学习过程中**<font style="color:#DF2A3F;">共享知识</font>**，利用**<font style="color:#DF2A3F;">任务间相关性</font>**来改进模型在每个任务上的性能和泛化能力

<p>可看做一种归纳迁移学习——利用包含在相关任务中的信息，作为归纳偏置，以提高泛化能力</p>
<h3 id="Ege7Y">2.常见的4种共享机制</h3>
<h4 id="L76Gy">（1）硬共享模式</h4>
不同任务的NN模型共用一些共享模块（**通常低层**）来提取**通用特征**

<p>不同任务再设置私有模块（<strong>通常高层</strong>）来提取任务<strong>特定特征</strong></p>
<h4 id="OkzBy">（2）软共享模式</h4>
不显式共享，而是允许每个任务从其他任务中**窃取**信息来提高自己的能力

<p>=&gt;窃取方式：</p>
<p>①直接复制使用其他任务的隐状态</p>
<p>②使用Attention机制主动选取有用的信息</p>
<h4 id="ocZOg">（3）层次共享模式</h4>
:::info
神经网络**不同层抽取的特征类型**：

<p>低层——低级的局部特征</p>
<p>高层——高级的抽象语义特征</p>
<p>:::</p>
<p>根据多任务学习中不同任务级别高低：</p>
<p>低级任务在低层输出</p>
<p>高级任务在高层输出</p>
<h4 id="iNt7K">（4）共享-私有模式</h4>
共享模块和任务特定（私有）模块分开

<p>共享模块——捕捉跨任务共享特征</p>
<p>私有模块——捕捉特定任务相关特征</p>
<h3 id="NwHWd">3.学习步骤</h3>
<h4 id="FzesQ">（1）联合训练阶段</h4>
①随机初始化模型参数

<p>②1······T轮迭代开始进行</p>
<p>③把每一个任务的训练集都随即划分小批量集合，划分数量<code>𝑐 = 𝑁&lt;sub&gt;𝑚&lt;/sub&gt;/𝐾&lt;sub&gt;𝑚&lt;/sub&gt;</code></p>
<p><code>𝑁&lt;sub&gt;𝑚&lt;/sub&gt;</code>为第<code>𝑚</code>个任务的训练集包含的样本数量</p>
<p><code>𝐾&lt;sub&gt;𝑚&lt;/sub&gt;</code>为第<code>𝑚</code>个任务的批量大小</p>
<p>④合并所有小批量样本，并随机排序打乱</p>
<p>⑤计算合并后的集合中，每个小批量样本上的损失，并更新参数</p>
<p>⑥继续迭代，直到结束，输出𝑚个模型</p>
<h4 id="XxBxd">（2）单任务精调阶段</h4>
刚才得到了参数，基于这些参数，在每个单独任务进行精调

<h3 id="cfgBZ">3.相较单任务学习泛化能力更好的原因</h3>
<h4 id="QN59U">（1）训练集更大（毕竟是多任务）</h4>
<h4 id="Dn6s9">（2）任务间有相关性（相当于隐式的数据增强）</h4>
<h4 id="gaaki">（3）避免过拟合到单个任务的训练集（共享需兼顾所有任务【相当于一种正则化】）</h4>
<h4 id="qAkhN">（4）获得了更好的表示（适用于多个不同任务）</h4>
<h4 id="Dq0HS">（5）可选择性利用其他任务学习到的隐藏特征</h4>
<h2 id="xrI3N">四、迁移学习</h2>
<h3 id="G1bsv">1.基础概念</h3>
<h4 id="oxga8">（1）领域`𝒟 = (𝒳, 𝒴, 𝑝(𝒙, 𝑦))`</h4>
<h5 id="SC1f1">①领域</h5>
一个样本空间及其分布

<h5 id="lBRws">②源领域`𝒟<sub>𝑆</sub>`</h5>
<h5 id="B1SRT">③目标领域`𝒟<sub>𝑇</sub>`</h5>
<h5 id="aS6kh">④领域间关系</h5>
        * 不同领域：输入空间不同 或 输出空间不同 或 概率分布不同
        * 源领域的样本数量一般远大于目标领域

<h3 id="YYJlO">2.思想</h3>
将相关任务的训练数据中的可泛化知识迁移到目标任务上

<p>（利用源领域中学习的知识来帮助目标领域上的任务）</p>
<h3 id="pJnk6">3.归纳迁移学习与转导迁移学习</h3>
<h4 id="yo4hp">（1）归纳学习与转导学习</h4>
归纳学习：最小化期望风险得到模型，一般的机器学习就是归纳学习

<p>转导学习：最小化给定测试集上的错误率，训练阶段可利用测试集信息</p>
<h4 id="vwP8G">（2）归纳迁移学习</h4>
<h5 id="S4n8O">①思想</h5>
源领域及其任务 --&gt; 学习出一般规律 --&gt;迁移到目标领域及其任务上

<h5 id="O4tNd">②要求</h5>
<h6 id="Ie5aO">a.源领域和目标领域相关</h6>
<h6 id="nrxxv">b.源领域有大量训练样本（可以有标注可以无标注）</h6>
            + 源领域有大量无标注数据时：

<p>源任务 </p>
<p>–&gt; 转换为无监督学习任务（eg.自编码/密度估计）</p>
<p>–&gt; 学习出可迁移表示 </p>
<p>–&gt; 迁移到目标任务</p>
<pre><code>        + 源领域有大量有标注数据时：
</code></pre>
<p>源任务 </p>
<p>–&gt; 学习训练出模型 </p>
<p>–&gt; 模型迁移到目标领域目标任务上</p>
<h5 id="pUvsh">③两种迁移方式</h5>
<h6 id="LDK3R">a.基于特征的方式</h6>
预训练模型的输出/中间隐藏层的输出 --&gt; 作为特征 --&gt; 直接加入到目标任务的学习模型中

<h6 id="U8onN">b.精调的方式</h6>
在目标任务上复用预训练模型的部分组件，并对其参数进行精调

<p>有针对性地选择预训练模型的不同层迁移到目标任务中</p>
<h5 id="ZfIg9">④预训练模型迁移的优点（相较于从零开始学习）</h5>
a.初始模型性能比随机初始化的模型好

<p>b.训练时模型的学习速度比从零开始学习快</p>
<p>c.模型最终性能更好，泛化性更好</p>
<h5 id="GrFlL">⑤与多任务学习的区别</h5>
a.多任务学习是同时学习多个任务，归纳迁移学习是分两个阶段的

<p>   （源任务上的学习阶段+目标任务上的迁移学习阶段）</p>
<p>b.多任务学习希望提高所有任务的性能，归纳迁移学习是单向知识迁移、只针对目标任务</p>
<h4 id="MVmpj">（3）转导迁移学习</h4>
<h5 id="whGwX">①思想</h5>
直接利用源领域和目标领域的样本进行迁移学习 

<h5 id="AVGfH">②常见子问题——领域适应</h5>
<h6 id="t5DvR">a.什么是领域适应问题</h6>
假设源领域和目标领域有相同的样本空间，但**数据分布不同**

<h6 id="TQGJ0">b.数据分布不一致的情况</h6>
            + 协变量偏移：两领域输入边际分布不同，但后验分布（即学习任务）相同
            + 概念偏移：两领域输入边际分布相同，但后验分布（即学习任务）不同
            + 先验偏移：两领域中输出标签的先验分布不同，条件分布相同

<p>:::info<br>协变量的概念：</p>
<p>可能影响预测结果的统计变量</p>
<p>机器学习中，可以看作输入</p>
<p>协变量偏移的概念：</p>
<p>输入在训练集和测试集上的分布不同</p>
<p>:::</p>
<h6 id="JQt5v">		c.领域适应问题要做什么</h6>
关键在于如何学习领域无关的表示！

<h6 id="ChETY">  d.如何学习领域无关的表示</h6>
            + 优化参数使得映射后的两个领域的输入分布差异最小

<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1731655469231-93cae5d4-0b14-4d57-a8e3-001ac120b81e.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1731655482050-1cc32515-db47-4b39-ad4e-32b7daeb3bd4.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1731655498958-cea3bf65-74ad-4676-a78f-5ff1e250c17c.png"></p>
<pre><code>        + 通过领域对抗学习
</code></pre>
<p>引入领域判别器<code>𝑐</code>来判断一个样本是来自于哪个领域</p>
<p>if <code>𝑐</code> 不能判断一个映射特征的领域信息，则是一个领域无关的表示</p>
<p>具体做法见下图</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1731654473639-580640f6-2a79-4ef8-930e-a50ec1813827.png"></p>
<h2 id="v8vFT">五、终身学习/持续学习</h2>
&gt; 像人类一样具有持续不断的学习能力，不因新知识忘记旧知识，反而用旧知识帮助学习新知识
&gt;

<h3 id="c8Agn">1.终身学习和其他学习的比较</h3>
<h4 id="wptOW">（1）终身学习vs归纳迁移学习</h4>
归纳迁移学习只关注优化目标任务性能，不关注知识的积累

<p>终身学习目标是持续的学习和知识积累</p>
<h4 id="V8G1N">（2）终身学习vs多任务学习</h4>
多任务学习在所有任务上同时进行联合学习

<p>终身学习是持续的一个一个学习</p>
<h3 id="EziGi">2.关键问题——如何避免灾难性遗忘</h3>
目前神经网络往往过参数化 --&gt; 一个任务有很多参数组合都可达到最好性能 --&gt; 选择一组不影响先前任务的参数

<h4 id="nO8iq">一个方法——弹性权重巩固</h4>
以两个任务的持续学习为例子

<p><img src="https://cdn.nlark.com/yuque/0/2024/jpeg/40781129/1731674378952-13d5f429-766f-4532-b9ee-6beb2d45df01.jpeg"></p>
<h5 id="b9pTt">①对Fisher信息矩阵部分进一步说明逻辑</h5>
我们用Fisher信息矩阵近似精度矩阵，是因为精度矩阵与协方差矩阵存在互为逆矩阵的关系。

<p>本质上想要的是协方差矩阵。</p>
<p>同时Fisher信息矩阵可以简化为对角矩阵，由Fisher信息矩阵对角线构成。</p>
<h5 id="YURBy">②那么什么是Fisher信息矩阵呢</h5>
性质：一种测量似然函数`𝑝(𝑥;𝜃)`携带的关于参数`𝜃`的信息量的方法

<p>怎么得出的：首先，有一个通常的事实——一个参数对分布的影响，可以通过对数似然函数的梯度衡量</p>
<p>然后，我们就让设计一个打分函数<code>𝑠(𝜃)</code>以符合这个事实，即<code>𝑠(𝜃) = ∇&lt;sub&gt;𝜃 &lt;/sub&gt;log 𝑝(𝑥; 𝜃)</code></p>
<p>其中，这个函数有个性质，是期望为0，证明略</p>
<p>最后，这个函数的协方差矩阵，就是Fisher信息矩阵，可以衡量参数<code>𝜃</code>的估计的不确定性</p>
<p>也就是：<code>𝐹(𝜃) = 𝔼[𝑠(𝜃)𝑠(𝜃)&lt;sup&gt;T&lt;/sup&gt;]= 𝔼 [∇&lt;sub&gt;𝜃&lt;/sub&gt; log 𝑝(𝑥; 𝜃) (∇&lt;sub&gt;𝜃&lt;/sub&gt; log 𝑝(𝑥; 𝜃))&lt;sup&gt;T&lt;/sup&gt;]</code></p>
<p>（解释一下，这里就是在算协方差，Cov=E[s<sup>2</sup>]-(E[s])<sup>2</sup>，又因为期望为0，所以只剩前面那项）</p>
<p>得到之后的问题：对于<code>𝐹(𝜃)</code>表达式，其中的似然函数<code>𝑝(𝑥;𝜃)</code>我们不知道具体形式，咋办呢？</p>
<p>可用经验分布估计：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1731675165688-e0da9d51-e692-4236-bff6-4d0302a2e68a.png"></p>
<p>然后怎么用：一方面，<code>𝐹(𝜃)</code>对角线的值反应<code>𝜃</code>在通过最大似然进行估计时的不确定性</p>
<p>–&gt;值大：则估计值方差越小，估计越可靠，携带关于数据分布的信息越多</p>
<p>–&gt;值小：则估计值方差越大，估计越不可靠，携带关于数据分布的信息越少</p>
<p>另一方面，可用作信息量的衡量，用下面这个公式来衡量</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1731675394726-a4de5b76-d73d-4e9e-a2e5-020e20eaf099.png"></p>
<p>还有，得到了训练任务<code>𝒯&lt;sub&gt;𝐵&lt;/sub&gt;</code>时的损失函数</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1731676099047-d3560297-1bce-4963-aa4b-57afb95d9630.png"></p>
<h2 id="SYQHd">六、元学习——学习的学习</h2>
&gt; 动态调整学习方式，像人脑一样，面对不同任务，自动找到对应的解决方式
&gt;

<h3 id="L8l8Y">1.目的</h3>
从已有任务中，学习一种学习方法或元知识，从而可以加速新任务的学习

<h3 id="y3OjP">2.与归纳迁移学习的区别</h3>
元学习更侧重从多种不同，甚至不相关的任务中，归纳出一种学习方法

<p>而归纳迁移学习有相关性要求，即要求源领域和目标领域相关</p>
<h3 id="uHfxi">3.和元学习比较相关的机器学习问题——小样本学习</h3>
每个类只有K个标住样本，其中K非常小

<p>if K=1，称为单样本学习</p>
<p>if K=0，称为零样本学习</p>
<h3 id="CK2D4">4.典型的元学习方法</h3>
<h4 id="EtjbO">（1）基于优化器的元学习</h4>
![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1731677774745-7f000bae-cb5d-415c-881e-a5d751abdf1d.png)

<h4 id="rB3ox">（2）模型无关的元学习 MAML</h4>
<h5 id="dtPSh">①做法</h5>
假设所有任务来自共同的一个任务空间

<p>–&gt; 利用这些任务学习一个通用表示（称为在所有任务上的元优化，也采用梯度下降进行优化，得到θ）</p>
<p>–&gt; 在特定单任务上，通过梯度下降来精调θ</p>
<h5 id="zBcfH">②目标</h5>
学习一个参数θ，这个θ经过梯度迭代，就可以在新任务上达到最好性能

<h5 id="Jjbor">③算法</h5>
可以看一下，其实和前面说的做法是一致的

<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1731678036284-fe6edbd4-185f-4da1-a78b-baf95b9f9aa5.png"></p>
<h1 id="bCqrh">第11章 概率图模型</h1>
&gt; 用图结构来描述多元随机变量间条件独立关系的概率模型
&gt;

<h2 id="axIrE">问题的导出</h2>
:::info
K维随机变量——联合概率为高维空间分布——难建模——需要参数量简直爆炸——怎么减少参数量



<p>为了减少参数量——做独立性假设——<code>𝑝(𝒙) ≜ 𝑃(𝑿 = 𝒙)  =  ∏  𝑝(𝑥&lt;sub&gt;𝑘&lt;/sub&gt;|𝑥&lt;sub&gt;1&lt;/sub&gt;, ⋯ , 𝑥&lt;sub&gt;𝑘−1&lt;/sub&gt;)</code>连乘起来</p>
<p>连乘的是每个分开的条件概率——通过条件依赖关系大大减少参数量</p>
<p>但变量数量太多时条件依赖关系复杂——图结构可视化概率模型</p>
<p>——描述条件独立性+复杂联合概率模型分解为简单条件概率模型的组合</p>
<p>:::</p>
<h3 id="kl5eO">1.图模型的三个基本问题</h3>
<h4 id="UJkor">表示问题：如何用图结构描述变量间依赖关系</h4>
<h4 id="xpQ1c">学习问题：（图结构的学习）和参数学习</h4>
<h4 id="f6sFI">推断问题：已知部分变量，计算其他变量条件概率分布</h4>
<h3 id="CQkru">2.图模型与机器学习非关系</h3>
很多机器学习模型可归结为概率模型

<h2 id="JoUGg">二、模型表示</h2>
<h3 id="HCPsg">1.要素</h3>
<h4 id="u4R1Q">节点：一组/一个随机变量	</h4>
<h4 id="NsJzI">边：随机变量间的概率依赖关系</h4>
<h3 id="m7fJd">2.分类</h3>
<h4 id="CqHfw">有向图模型</h4>
        * 有向非循环图
        * 有连边——两变量有因果关系——不存在其他随机变量使得这俩变量条件独立

<h4 id="PLOUK">无向图模型</h4>
        * 无向图
        * 有连边——两变量有概率依赖关系，不一定因果关系

<h3 id="NRJ0h">3.有向图模型=贝叶斯网络=信念网络</h3>
<h4 id="uhSkM">（1）两个节点直接连接</h4>
它们必然非条件独立，且是直接因果关系——父节点是因，子结点是果

<h4 id="bLLvk">（2）两个节点不直接连接，但经过其他节点可达</h4>
<h5 id="obDQH">①间接因果关系</h5>
<h5 id="EpbIc">②间接果因关系</h5>
<h5 id="H9K0M">③共因关系：<font style="color:#DF2A3F;">因</font>X<sub>2</sub>已知，则<font style="color:#DF2A3F;">独立</font></h5>
<h5 id="i8eAB">④共果关系：<font style="color:#DF2A3F;">果</font>X<sub>2</sub>已知，则<font style="color:#DF2A3F;">不独立</font></h5>


<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1732105043856-3274b6f5-ae59-4501-96fb-e1373c2a0a76.png"></p>
<p>参照这张图来理解，其中<strong>x</strong><sub><strong>2</strong></sub>是中间节点</p>
<h4 id="EJrbG">（3）局部马尔可夫性</h4>
给定父节点，则每个随机变量要条件独立于它的非后代节点

<pre><code>    * 后代节点可以有关系
    * 但是非后代节点得条件独立
</code></pre>
<h4 id="kkqaN">（4）常见的有向图模型</h4>
<h5 id="x8CGs">①Sigmoid信念网络（SBN）</h5>
![](https://cdn.nlark.com/yuque/0/2024/jpeg/40781129/1732106295267-ee842e09-d5b5-412a-84bc-2fd80eaaee18.jpeg)

<p>与Logistic回归模型的区别：</p>
<pre><code>        + **𝒙**的本质：Logistic回归模型的**𝒙**是确定性参数而不是变量
</code></pre>
<p>  Sigmoid信念网络的<strong>𝒙</strong>是随机变量</p>
<pre><code>        + 模型本质：Logistic 回归模型只建模条件概率 𝑝(𝑦|𝒙)，是一种**判别模型**

      Sigmoid信念网络建模联合概率 𝑝(𝒙, 𝑦)，是一种**生成模型**
</code></pre>
<h5 id="eiqir">②朴素贝叶斯分类器（NB）</h5>
原理：贝叶斯公式+强独立性假设（朴素所在）---&gt; 计算每个类别条件概率

<p><code>𝑝(𝑦|𝒙;𝜃) ∝ 𝑝(𝑦|𝜃&lt;sub&gt;𝑐&lt;/sub&gt;)  ∏  𝑝(&lt;font style="color:#DF2A3F;"&gt;𝑥&lt;/font&gt;&lt;sub&gt;&lt;font style="color:#DF2A3F;"&gt;𝑚&lt;/font&gt;&lt;/sub&gt;|𝑦;𝜃&lt;sub&gt;𝑚&lt;/sub&gt;)</code></p>
<p>若<code>&lt;font style="color:#DF2A3F;"&gt;𝑥&lt;/font&gt;&lt;sub&gt;&lt;font style="color:#DF2A3F;"&gt;𝑚&lt;/font&gt;&lt;/sub&gt;</code>是连续值——可用高斯分布建模</p>
<p>若<code>&lt;font style="color:#DF2A3F;"&gt;𝑥&lt;/font&gt;&lt;sub&gt;&lt;font style="color:#DF2A3F;"&gt;𝑚&lt;/font&gt;&lt;/sub&gt;</code>是连续值——可用多项式分布建模</p>
<h5 id="PAIDP">③隐马尔可夫模型（HMM）</h5>
所有隐变量构成一个马尔可夫链

<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1732183548199-c6935779-03e7-4ebb-9e9f-e7d92c4add58.png"></p>
<p><code>𝑝(𝒙, 𝒚;𝜃) =  ∏ 𝑝(𝑦&lt;sub&gt;𝑡&lt;/sub&gt;|𝑦&lt;sub&gt;𝑡−1&lt;/sub&gt;, 𝜃&lt;sub&gt;𝑠&lt;/sub&gt;) 𝑝(𝑥&lt;sub&gt;𝑡&lt;/sub&gt;|𝑦&lt;sub&gt;𝑡&lt;/sub&gt;, 𝜃&lt;sub&gt;𝑡&lt;/sub&gt;)</code></p>
<p>  转移概率		    输出概率</p>
<p><strong>《机器学习方法》中关于隐马尔可夫模型的内容：</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.yuque.com/yuqueyonghucoit3e/wefx9h/advx3ocg32wlbzhs">第10章 隐马尔可夫模型</a></p>
<h3 id="ozXvx">4.无向图模型=马尔可夫随机场=马尔可夫网络</h3>
<h4 id="PZLyk">（1）定义</h4>
用无向图来描述一组具有局部马尔可夫性质的随机向量 𝑿 的联合概率分布的模型

<p>（也就是具备局部马尔可夫性的无向图）</p>
<h4 id="SLHmd">（2）无向图的局部马尔可夫性</h4>
𝑋<sub>𝑘 </sub>条件独立于 除了自己和邻居 以外的 所有节点

<h4 id="hFiCs">（3）无向图模型的概率分解</h4>
<h5 id="qqGUB">①团</h5>
无向图中的一个全连通子图——所有节点都有边相连 所形成的一坨

<h5 id="LaiMp">②最大团</h5>
不能被其他团包含了 就是最大团

<p>当然 如果再加一个节点就不满足全连通性 也就是最大团</p>
<h5 id="HJcCt">③因子分解</h5>
分布`𝑝(𝒙) &gt; 0`满足无向图中的局部马尔可夫性

<p>=&gt;<code>𝑝(𝒙)</code>可表示为<code>𝑝(𝒙) = 1/𝑍 ∏ 𝜙&lt;sub&gt;𝑐&lt;/sub&gt;(𝒙&lt;sub&gt;𝑐&lt;/sub&gt;)</code> ——吉布斯分布</p>
<pre><code>            - `𝜙&lt;sub&gt;𝑐&lt;/sub&gt;(𝒙&lt;sub&gt;𝑐&lt;/sub&gt;) ≥ 0 `是定义在团 𝑐 上的势能函数
            - 配分函数 ![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1732186231573-da14ef8a-40ec-4415-b16a-3d5cfae46907.png) 用来归一化乘积为概率形式
</code></pre>
<h5 id="hdDyP">④吉布斯分布</h5>
`𝑝(𝒙) = 1/𝑍 ∏ 𝜙<sub>𝑐</sub>(𝒙<sub>𝑐</sub>)` ，其中一般定义`𝜙<sub>𝑐</sub>(𝒙<sub>𝑐</sub>) = exp(−𝐸<sub>𝑐</sub>(𝒙<sub>𝑐</sub>))`且`𝐸<sub>𝑐</sub>(𝒙<sub>𝑐</sub>)` 为能量函数

<pre><code>这个形式称为吉布斯分布                                              这样定义后，分布又称为玻尔兹曼分布
</code></pre>
<p>性质是：吉布斯分布一定满足马尔科夫随机场条件独立性质</p>
<p>马尔科夫随机场的概率分布一定可以表示成吉布斯分布</p>
<h4 id="JvO88">（4）常见的无向图模型</h4>
<h5 id="U8f2w">①对数线性模型=条件最大熵模型=Softmax回归模型</h5>
刚才提到的吉布斯分布`𝑝(𝒙) = 1/𝑍 ∏ 𝜙<sub>𝑐</sub>(𝒙<sub>𝑐</sub>)` ，让其中的`𝜙<sub>𝑐</sub>(𝒙<sub>𝑐</sub>)`为：

<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1732431840066-0d4487a7-8478-4f3b-a826-5a9d9cc58d69.png"></p>
<p>然后取对数：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1732431951765-c0306155-024d-4288-8e93-dd73661e4f36.png"></p>
<p><code>𝑓&lt;sub&gt;𝑐&lt;/sub&gt;(𝒙&lt;sub&gt;𝑐&lt;/sub&gt;)</code> 为定义在 <code>𝒙&lt;sub&gt;𝑐&lt;/sub&gt; </code>上的特征向量；<code>𝜃&lt;sub&gt;𝑐&lt;/sub&gt;</code> 为权重向量</p>
<p>然后得到 <code>𝑝(𝑦|𝒙;𝜃)</code>：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1732432705223-a1a99212-9e15-4b56-bdfa-cfc0c6fbcbbf.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1732432717916-a87ea1e0-b49e-4c05-b9c0-99de5a5cfc8e.png"></p>
<p><strong>《机器学习方法》中关于最大熵模型的内容：</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.yuque.com/yuqueyonghucoit3e/wefx9h/kuyviah06zprz9yd">第6章 逻辑斯谛回归与最大熵模型</a></p>
<h5 id="vo5e5">②条件随机场</h5>
对y要有分解，即：

<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1732433459453-fd2bb0b8-a667-4372-a7f6-a5017304ef74.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1732433513466-f481cd6b-87f5-46ea-9916-a5cd2037f8d3.png"></p>
<p>（可以对比最大熵模型的条件概率公式，可以发现做了分解）</p>
<p>线性链条件随机场的条件概率：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1732433649652-f447f7b7-762b-47d0-9e94-f5393ffa3188.png"></p>
<p><strong>《机器学习方法》中关于条件随机场的内容：</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.yuque.com/yuqueyonghucoit3e/wefx9h/sqqfozz09h4bxwmi">第11章 条件随机场</a></p>
<h5 id="HaBv0">③玻尔兹曼机</h5>
见第12章

<h5 id="jSYWl">④受限玻尔兹曼机</h5>
见第12章

<h4 id="EH0dn">（5）有向图和无向图间的转换</h4>
<h5 id="ANyjC">①为啥</h5>
有向图转无向图，可以利用无向图上的精确推断算法，同时也可以表示有向图无法表示的一些**<font style="color:#DF2A3F;">依赖关系</font>**

<p><strong><font style="color:#DF2A3F;">（注意不是所有关系，比如无向图是不能表示因果关系的）</font></strong></p>
<h5 id="ijXOF">②过程--道德化</h5>
看图理解就行：

<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1732435783230-e206844d-6501-403d-a437-956c2d752a55.png"></p>
<h2 id="cIrVm">三、模型的学习</h2>
<h3 id="eVv2X">1.分类</h3>
寻找最优网络结构（难，专家构建）+ 已知网络结构估计每个条件概率分布参数（按包不包含隐变量分两种）

<h3 id="OShiy">2.不含隐变量的参数估计——最大似然</h3>
<h4 id="P6ZKa">（1）有向图模型</h4>
<h5 id="MKD0U">①第一步，构造对数似然</h5>
![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1732437128923-b4025d45-88d4-4a46-a7b7-41044978fe06.png)

<p>从11.27到11.28，是因为：</p>
<p>”所有变量 𝒙 的联合概率分布可分解为每个随机变量 𝑥<sub>𝑘</sub> 的局部条件概率 𝑝(𝑥<sub>𝑘</sub>|𝑥<sub>𝜋𝑘</sub> ; 𝜃<sub>𝑘</sub>) 的连乘形式“</p>
<p>说人话就是：</p>
<p>可以拆成部分连乘形式，然后log一下，就变成了累加~</p>
<h5 id="sag9l">②第二步，最大化对数似然`ℒ(𝒟; 𝜃)`</h5>
转化为分别最大化每个变量的条件似然，即：

<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1732450345026-e30f3f9d-ceb1-4576-af2e-f78d58f2e07b.png"></p>
<pre><code>    * 如果x离散——使用参数化模型减少参数量——例如用Sigmoid信念网络
    * 如果x连续——用高斯函数表示条件概率分布——高斯信念网络
</code></pre>
<h4 id="sWR8X">（2）无向图模型</h4>
<h5 id="tyWpf">①第一步，构造对数似然</h5>
![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1732451547729-41a915a2-a496-46fc-ae09-7dedb9ca4d57.png)

<p>这里直接就代入了对数线性模型的概率公式：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1732451683933-c6c5f730-d40e-4f77-81d5-b62fdda40513.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1732451713638-d58634f0-0c33-4468-84a0-fb7917505bde.png"></p>
<h5 id="l259t">②第二步，最大化对数似然`ℒ(𝒟; 𝜃)`</h5>
用梯度上升方法：

<p>求关于𝜃<sub>𝑐</sub> 的偏导数，推导过程如下</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1732451927296-28f92d00-455b-4d2f-8bdc-45eabd30742c.png"></p>
<p>最优点梯度为0，也就是式11.37等于0，从而也就是书上说的“优化目标等价于：对于每个团 𝑐 上的特征 𝑓<sub>𝑐</sub>(𝒙<sub>𝑐</sub>)，使得其在经验分布  ̃𝑝(𝒙) 下的期望等于其在模型分布 𝑝(𝒙; 𝜃) 下的期望”</p>
<p>式11.37的后半部分（在模型分布下的期望）很难算，通常采用近似，有两种方式：</p>
<p>a.利用采样来近似计算期望</p>
<p>b.坐标上升法：固定其他参数，优化一个势能函数的参数</p>
<h3 id="sn4iJ">3.含隐变量的参数估计</h3>
<h4 id="l1P7s">（1）EM算法</h4>
&gt; 这里同样也是从最大化对数似然的原理出发得到的EM算法
&gt;

<p><strong>逻辑</strong>：首先我们有一个样本 𝒙 的边际似然函数（i.e.证据）</p>
<p>   然后构造对数似然，和前面”不含隐变量的参数估计“部分的公式原理相同，长下面这样</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1732453050356-b3b73707-88ab-47d2-ac16-b22a35378edd.png"></p>
<p>   然后我们要最大化对数似然，但涉及到 𝑝(𝑥) 的推断问题（导致对数函数内部的求和/积分去不掉且难算）</p>
<p>   然后我们就引入变分函数𝑞(𝒛)（定义在隐变量 𝒁 上的分布）来解决问题，得到下面这样</p>
<pre><code>![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1732453192935-083b22ef-2d25-4118-891e-48487015a778.png)
</code></pre>
<p>   所以最大化似然函数就转化为了两步<u><font style="color:#DF2A3F;">的不断重复直到收敛到局部最优解</font></u>：</p>
<h5 id="bEI0M">  		①E步</h5>
固定参数 𝜃<sub>𝑡</sub>，找到一个分布 𝑞<sub>𝑡+1</sub>(𝒛) 使得证据下界 𝐸𝐿𝐵𝑂(𝑞, 𝒙; 𝜃<sub>𝑡</sub>) 等于 log 𝑝(𝒙; 𝜃<sub>𝑡</sub>)

<p>其中，最理想有<code>𝑞(𝒛) = 𝑝(𝒛|𝒙,𝜃&lt;sub&gt;𝑡&lt;/sub&gt;)</code>，因为此时<code>𝐸𝐿𝐵𝑂(𝑞&lt;sub&gt;𝑡+1&lt;/sub&gt;, 𝒙; 𝜃)</code>最大</p>
<p>但是，如果𝒛的维度较高，<code>𝑝(𝒛|𝒙,𝜃&lt;sub&gt;𝑡&lt;/sub&gt;)</code>不好算，需要变分推断方法</p>
<h5 id="Jyivc">  ②M步</h5>
固定 𝑞<sub>𝑡+1</sub>(𝒛)，找到一组参数使得证据下界最大

<p><code>𝜃&lt;sub&gt;𝑡+1&lt;/sub&gt; = arg max &lt;sub&gt;𝜃&lt;/sub&gt; 𝐸𝐿𝐵𝑂(𝑞&lt;sub&gt;𝑡+1&lt;/sub&gt;, 𝒙; 𝜃)</code></p>
<p><strong>收敛性证明略</strong></p>
<p><strong>信息论视角推导出EM算法略</strong></p>
<p><strong>《机器学习方法》中关于EM算法的内容：</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.yuque.com/yuqueyonghucoit3e/wefx9h/gzld9qugrwi4yp08">第9章 EM算法及其推广</a></p>
<h2 id="UVy30">四、模型的推断</h2>
<h3 id="Z0QHd">1.推断</h3>
<h4 id="cvFJg">（1）概念</h4>
观测到部分变量时，计算其他变量的某个子集的条件概率

<p>=&gt;关键在：求任意一个变量子集的边际分布</p>
<h4 id="d4pVF">（2）分类</h4>
精确推断、近似推断

<h3 id="VWDDy">2.精确推断</h3>
<h4 id="uXRIe">（1）变量消除法</h4>
`p(x<sub>1</sub>|x<sub>4</sub>)=p(x<sub>1</sub> , x<sub>4</sub>) / p(x<sub>4</sub>)`

<p>其中：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734680575646-037268e4-d3bf-4733-bf75-f4ec9b5c7116.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734681007597-d391be22-debf-4405-8120-95b44abd8803.png"></p>
<p>这个过程中消除了变量，减少计算边际分布的计算复杂度</p>
<h4 id="ZFoqV">（2）信念传播算法=BP算法=和积算法=消息传递算法</h4>
思想：将变量消除法的和积操作看作消息并保存

<p>消息传递过程：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734681770197-a0f0547a-f35f-45c4-b225-24ceb13df0a1.png"></p>
<h5 id="ZEQly">树结构上的信念传播算法</h5>
![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1734682085914-cd956ed9-69be-4b24-a7b2-b3736e6f51a0.png)

<h3 id="E6KUZ">3.近似推断</h3>
if图模型结构复杂=&gt;精确推断开销大

<p>if图模型变量连续+积分函数没有闭式解=&gt;无法使用精确推断</p>
<h4 id="u4KD4">（1）环路信念传播</h4>
在具有环路的图上用信念传播算法

<h4 id="VN20P">（2）变分推断</h4>
引入简单分布（称为变分分布）近似条件概率，迭代计算：

<p>更新变分分布参数，最小化变分分布和真实分布的差异=&gt;根据变分分布来推断</p>
<h4 id="kM5du">（3）采样法</h4>
用模拟方式采集符合某个分布的样本，用样本估计和分布有关的运算

<h2 id="h9kWI">五、变分推断</h2>
泛函：函数的函数，输入是函数，输出是实数=&gt; f(x)的泛函：F(f(x))

<p>变分法：寻找使F(f(x))取得极大/极小值的f(x)</p>
<p>变分推断：推断问题转换为泛函优化问题——<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734685101039-de38636a-ae90-4b6a-97bd-3f70ef45bdbc.png">最小化变分分布和真实分布差异</p>
<p>这个问题的解法：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734686049345-b34320f9-b5a1-4e84-a7ed-9d1a03a11cee.png"></p>
<h2 id="FvcQ5">六、基于采样法的近似推断</h2>
![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1734790562062-5efb041b-1a24-47f0-a957-2a454d9d2963.png)	其中要推断的概率分布为p(x)，采样法近似计算这个期望

<h3 id="cCl42">1.采样法=蒙特卡罗方法=统计模拟方法</h3>
通过随机采样（给定概率密度函数中抽取符合其概率分布的样本）近似估计计算问题数值解

<p>过程：p(x)中独立抽取N个样本 =&gt; N个样本的均值近似为f(x)的期望 =&gt; N无穷大时收敛于期望值</p>
<p>难点：如何让计算机生成满足p(x)的样本</p>
<p>解决方案：用p(x)的累积分布函数cdf(x)的逆函数来生成服从p(x)的样本</p>
<p>难点：累积分布函数cdf(x)的逆函数在p(x)复杂时难以计算</p>
<p>解决方案：见下</p>
<h3 id="MrdyV">2.拒绝采样=接受-拒绝采样</h3>
p(x)难以采样 =&gt; 引入容易的采样的分布q(x) 称为提议分布 =&gt; 以某**<font style="color:#DF2A3F;">标准</font>**拒绝一部分样本 =&gt; 最终采集的样本浮层p(x) 【kq(x)覆盖未归一化的分布p(x)——kq(x)≥p(x)】

<p>**<font style="color:#DF2A3F;">标准</font>**：每次抽取样本的接受概率</p>
<p>   <img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734791678715-f8c8ae40-382b-4433-bb27-77d5161e98c2.png"></p>
<p>判断拒绝采样的好坏：拒绝率太高，采样效率会不理想</p>
<h3 id="yoQOG">3.重要性采样</h3>
思想：我只是想算那个期望（𝔼𝑝[𝑓(𝑥)]），抽取的样本其实不用严格服从p(x)，所以用提议分布q(x)直接采样并估计期望

<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734792317276-822ffe69-1c8f-40b2-98da-686e4e54ff43.png"></p>
<p>w(x)是重要性权重</p>
<h3 id="oGTmP">4.马尔可夫链蒙特卡罗方法 MCMC</h3>
思想：将采样过程看成马尔可夫链——第t+1次采样依赖于第t次抽取的样本和状态转移分布（即提议分布）

<p>关键：构造出平稳分布为p(x)的马尔可夫链，且状态转移分布容易采样</p>
<p>预烧期：马尔可夫链需要一段时间的随机游走才能达到平稳状态，这个时期内的采样点要丢掉！</p>
<h4 id="OIFcY">（1）Metropolis-Hastings算法=MH算法</h4>
动机：状态转移分布（提议分布）的平稳分布往往不是p(x)

<p>思想：引入拒绝采样来修正提议分布，使最终采样的分布为p(x)</p>
<p>与拒绝采样的区别：第t+1次采样样本的接受概率和第t次所采样的样本有关系！</p>
<h4 id="s9j2G">（2）Metropolis算法</h4>
在MH算法基础上，加限定条件：提议分布是对称的

<p>这样使得接受率公式简化了！</p>
<h4 id="FydVD">（3）吉布斯采样</h4>
可看作MH算法的特例

<p>思想：使用全条件概率作为提议分布，对每个维度采样，设置接受率A=1</p>
<p>全条件概率：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734793771706-409738d2-dadf-4114-a237-052294c00b99.png"></p>
<p>每单步采样构成马尔可夫链。</p>
<h3 id="nAKdB">5.几种方法的对比</h3>
拒绝采样、重要性采样——效率随空间维数的增加而指数降低

<p>马尔可夫链蒙特卡罗方法——容易对高维变量进行采样</p>
<h1 id="O8b7I">第12章 深度信念网络</h1>
<h2 id="KScF4">一、玻尔兹曼机</h2>
<h3 id="NLw3k">1.形式</h3>
看图理解

<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734796524691-e180d359-03a2-4bfc-a923-5b04a03d0766.png"></p>
<h3 id="n03kA">2.玻尔兹曼分布</h3>
![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1734796646984-6711063e-c282-4808-b498-e0be102a6b2a.png)

<p>𝐸<sub>𝛼</sub> 为状态 𝛼 的能量，𝑘 为玻尔兹曼常量，𝑇 为系统温度,  exp( −𝐸<sub>𝛼</sub> /𝑘𝑇 ) 称 为玻尔兹曼因子</p>
<h3 id="MlLI2">3.玻尔兹曼机</h3>
<h4 id="hVSyU">（1）概念</h4>
![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1734797899548-e18f5c9f-e28f-4b39-af6c-9a589f7f8f8c.png)

<p>x<sub>i</sub> 取值1表示模型接受x<sub>i</sub>代表的假设，取值0代表模型拒绝x<sub>i</sub>代表的假设</p>
<p>w<sub>ij</sub>表示两个假设间的弱约束关系，为正说明相互支持，可能被同时接受，为负反之</p>
<h4 id="ubRPJ">（2）应用</h4>
<h5 id="wwcAY">①解决搜索问题</h5>
<h5 id="smofE">②解决学习问题</h5>
<h3 id="j9a6M">4.生成模型——以基于吉布斯采样生成样本为例</h3>
过程：

<h5 id="rQTlI">①随机选择变量X<sub>i</sub></h5>
<h5 id="gvgH3">②根据全条件概率设置状态为1或0</h5>
![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1734799154446-6072a38d-1108-4b19-a2dc-9a5cb1731641.png)

<h5 id="C7uzM">③固定温度T下运行足够时间，达到热平衡</h5>
T → ∞ ： 可以很快达到热平衡

<p>T → 0 ： 变成确定性方法，退化成Hopfield网络</p>
<h5 id="qR6BQ">④此时任何全局状态概率服从玻尔兹曼分布p(x)，只与系统能量有关，与初始状态无关</h5>
<h3 id="MZjhz">5.能量最小化与模拟退化</h3>
<h4 id="GtL6G">（1）能量最小化</h4>
简单、确定性方法如Hopfield网络，会收敛到局部最优解而不是全局最优

<h4 id="N3dX1">（2）模拟退火</h4>
目的：作为寻找全局最优的近似方法

<p>过程：刚开始在高温运行达到热平衡，然后逐渐降低，直到低温也达到热平衡</p>
<h3 id="a2lo5">6.参数学习</h3>
学习的参数是：W和b

<p>定义对数似然函数后求偏导</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734807044635-9de356f5-3af0-4fec-b59e-d49ddcb13f31.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734808682577-bc3b2a42-1820-421a-ae19-93a334c27a2a.png"></p>
<p>涉及计算配分函数和期望，难精确计算，所以用MCMC方法来近似求解：</p>
<p>固定住可观测变量 𝒗，只对 𝒉 进行吉布斯采样</p>
<p>=&gt; 达到热平衡，采样x<sub>i</sub> x<sub>j</sub>的值</p>
<p>=&gt; 近似期望</p>
<h2 id="AQ5OT">二、受限玻尔兹曼机</h2>
玻尔兹曼机每更新一次权重就要重新热平衡，低效

<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734809842162-64f42c0f-ccf5-4c69-8066-bcfdfa3e5a17.png"></p>
<p>规定能量函数和联合概率分布：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734852124541-96fdb738-0882-4501-a5b9-398283731918.png"></p>
<h3 id="iEkwR">1.生成模型</h3>
给定联合分布概率：

<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734853364005-2febe201-d902-4fe3-aaf9-233d7275c700.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734853904348-77c5b7ec-32d4-493b-b1af-31d94b58a9b7.png"></p>
<p>吉布斯采样：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734855168838-f32d84b2-4fd9-48cf-a21c-f74e2e666832.png"></p>
<h3 id="q6TSW">2.参数学习</h3>
要学习的参数：W a b

<p>方法：最大化对数似然函数</p>
<p>   <img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734855677926-171af57b-16dd-46fd-a364-419911fdffb6.png"></p>
<p>   求偏导</p>
<p>   <img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734855713346-93110036-b3a6-4eee-a7b9-d93b1d070cd4.png"></p>
<p>   要算配分函数和俩期望，难算，用MCMC方法近似：</p>
<pre><code> 固定v并设为训练样本中的值，根据条件概率对h采样，

 再不固定v，通过**吉布斯采样**轮流更新v和h，达到热平衡采集v和h的值
</code></pre>
<p>  <img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734855819640-768a7acf-0968-4fe2-a676-faf5cf39fe6e.png"></p>
<h4 id="gXYha">对比散度学习算法（对受限玻尔兹曼机来说，比吉布斯采样更有效）</h4>
【CD-k算法】

<p>用一个训练样本作v的初始值 =&gt; 交替对v和h进行吉布斯采样 =&gt;不需要等到收敛，k步足够</p>
<h3 id="ZgWPx">3.受限玻尔兹曼机的类型</h3>
<h4 id="Fipyq">（1）伯努利-伯努利 受限玻尔兹曼机</h4>
<h4 id="HXlQJ">（2）高斯-伯努利 受限玻尔兹曼机</h4>
<h4 id="AA43k">（3）伯努利-高斯 受限玻尔兹曼机</h4>
<h2 id="Ui3lE">三、深度信念网络 DBN</h2>
![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1734856597048-732159c8-0c9f-4db3-8cf0-f78330bf0f5c.png)

<p>有：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734856848614-401440eb-a9ae-4aa0-84ea-2c0a2f866f3e.png">且<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734856893382-4db6b3ea-5aac-42a6-aa01-cd723b50db0b.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734856877023-7337507f-9ea6-440c-be02-d85a4a1d7cba.png"></p>
<h3 id="JzuFr">1.生成模型</h3>
生成样本过程：

<p>运行顶层受限玻尔兹曼机，进行足够多次吉布斯采样</p>
<p>=&gt;达到热平衡，生成样本h<sup>(L-1)</sup></p>
<p>=&gt;计算下一层变量条件分布+采样</p>
<p>=&gt;continue</p>
<h3 id="uB3X3">2.参数学习</h3>
过程：

<p>每层Sigmoid信念网络转化成受限玻尔兹曼机【隐变量后验概率相互独立】</p>
<p>=&gt;逐层训练【自底向上，每次训练一层，训练包括俩阶段：逐层预训练、精调】</p>
<h4 id="Co8yZ">（1）逐层预训练</h4>
<h4 id="RVMND">![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1734857888794-e31c4cbc-c4b3-4c9d-b9d0-217ec34bf59f.png)</h4>
<h4 id="rStTu">（2）精调</h4>
<h5 id="FcBw4">①作为生成模型的精调</h5>
![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1734858258324-4703410a-0bd8-419e-bd3f-16006ab8aa60.png)

<p>生成权重：向下的 | 定义原始生成模型</p>
<p>认知权重：向上的 | 反向计算上行的条件概率</p>
<h6 id="n4hG5">Wake-Sleep算法</h6>
![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1734870461235-e5203462-6c66-4b02-bc57-a87799f05541.png)

<p>交替两个阶段直到收敛</p>
<h5 id="BpbKs">②作为判别模型的精调</h5>
在深度信念网络的最顶层再增加一层输出层，然后使用 反向传播算法 对这些权重进行调优

<h1 id="SOC8P">第13章 深度生成模型</h1>
概率生成模型=生成模型：

<p>根据可观测样本，学习参数化模型，<strong>近似未知分布</strong>，<strong>生成与真实样本相近的样本</strong></p>
<pre><code>     &lt;font style="color:#DF2A3F;"&gt;概率密度估计    &lt;/font&gt;            &lt;font style="color:#DF2A3F;"&gt;生成样本（采样）&lt;/font&gt;
</code></pre>
<p>深度生成模型：</p>
<p>利用深度神经网络建模复杂分布/生成符合分布的样本</p>
<h2 id="Uk3UT">一、概率生成模型</h2>
<h3 id="Mo5jo">1.密度估计</h3>
概念：根据数据集估计它的概率密度函数pde

<p>难点：不存在复杂依赖关系，难用图模型</p>
<p>解决：引入隐变量，用EM算法</p>
<p>问题：EM算法需要估计的条件分布比较复杂的时候咋整？</p>
<p>解决：变分自编码器——用神经网络建模</p>
<h3 id="C4Vct">2.生成样本=采样</h3>
概念：给定pde，生成相应样本

<p>过程：在EM算法中，隐变量先验分布采样+条件分布采样</p>
<p>生成对抗网络的思想：从简单分布中采集出的样本，送到神经网络里，使得输出服从我们的分布，避免了密度估计</p>
<h3 id="ELtoe">3.应用于监督学习</h3>
典型：朴素贝叶斯、隐马尔可夫

<p>on opposite：判别模型 eg.Logistic回归、SVM、神经网络</p>
<p>关系：生成模型可得到判别模型，反之不成立</p>
<h2 id="ho4Xg">二、变分自编码器</h2>
为了得到，拆解为：![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872132020-29cfc142-d992-4720-8a53-6564c6d4d86b.png)

<p>假设拆解后的这两个分布都服从某种参数化分布族，则用最大似然来估计</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872212071-28d2e2b9-7b07-4cf0-8378-a64ef61d194f.png">		</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872221422-4a026eed-9c6c-412e-8793-07354e29e21b.png"></p>
<p>其中<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872245297-dff73eee-ee45-4229-bd70-b090bf0554bf.png">是额外引入的变分密度函数</p>
<p>最大似然过程用EM：</p>
<p>E：寻<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872327662-eaa2c392-5837-4277-837a-d1a9a831512d.png">等于/接近<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872347092-b0ea8dc8-cdeb-46e0-ace0-ac7fffeeada0.png"></p>
<p>M：固定<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872327662-eaa2c392-5837-4277-837a-d1a9a831512d.png">，找θ，最大化ELBO</p>
<p>理论最优<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872327662-eaa2c392-5837-4277-837a-d1a9a831512d.png">=<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872347092-b0ea8dc8-cdeb-46e0-ace0-ac7fffeeada0.png">：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872406231-9a674c89-3979-47ec-9912-78154f69f72a.png">，但这个<strong>不好计算</strong></p>
<p>解决方案：变分推断，近似估计，用简单的<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872327662-eaa2c392-5837-4277-837a-d1a9a831512d.png">来近似推断<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872347092-b0ea8dc8-cdeb-46e0-ace0-ac7fffeeada0.png"></p>
<p>问题：<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872347092-b0ea8dc8-cdeb-46e0-ace0-ac7fffeeada0.png">一般复杂，近似效果不好，也难用已知分布族函数建模</p>
<p>解决方案：变分自编码器</p>
<p>解决具体方式：</p>
<p>用神经网络估计<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872327662-eaa2c392-5837-4277-837a-d1a9a831512d.png">（近似<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872347092-b0ea8dc8-cdeb-46e0-ace0-ac7fffeeada0.png">）——这个神经网络称为推断网络——输入x，输出<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734874168597-1067d6f0-e503-4a90-869a-8483f7d66f38.png"></p>
<p>【推断网络的目标：<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734874168597-1067d6f0-e503-4a90-869a-8483f7d66f38.png">尽可能接近真实后验<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872347092-b0ea8dc8-cdeb-46e0-ace0-ac7fffeeada0.png"> =&gt; 最小化KL =&gt; 找到网络参数**<font style="color:#DF2A3F;">𝜙</font><strong><sup></sup></strong><font style="color:#DF2A3F;">∗</font>**使得ELBO最大】</p>
<pre><code>   用神经网络估计![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872812773-687a53bd-48f6-4afe-a15c-49425ffef82a.png)——这个神经网络称为生成网络——输入z，输出![](https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872799769-2ccebd70-c4ed-43da-9454-6839272596b3.png)
</code></pre>
<p>（<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734874588271-a79e2d0d-8c05-4153-854f-e73106b4d042.png">可分为 隐变量 𝒛 的先验分布<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734874549927-c94f5f9c-81b7-44ad-86a6-99d7e06cb7f7.png">和条件概率分布<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734874562219-714d1fc3-ade9-4f79-9efb-e3d57bd9fc3e.png">）</p>
<p>【生成网络的目标：找到网络参数**<font style="color:#DF2A3F;">θ</font><strong><sup></sup></strong><font style="color:#DF2A3F;">∗</font>**使得ELBO最大】</p>
<p>  【注意<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872347092-b0ea8dc8-cdeb-46e0-ace0-ac7fffeeada0.png">和<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734872812773-687a53bd-48f6-4afe-a15c-49425ffef82a.png"> 和<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734874168597-1067d6f0-e503-4a90-869a-8483f7d66f38.png">not the same】</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734873905175-904363a2-402f-441c-810b-ef9494379b1b.png"></p>
<p>汇合推断网络和生成网络的目标（都是使证据下界ELBO最大）：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734875204678-2b90be01-0b9c-43a0-b6f7-f6509ccb8bb5.png"><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734875222457-4ad1c8b5-e26a-4b1c-a7ee-e9f4985240d6.png"></p>
<p>再参数化：𝑓(𝜃) 的参数 𝜃=𝑔(𝜗)，则 𝑓(𝜗)= 𝑓(𝑔(𝜗))</p>
<p>引入分布为𝑝(𝜖) 的随机变量 𝜖，把目标的第一项期望写成<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734875854657-2c217ee0-93ea-450a-ac65-9af9ab09de19.png"></p>
<p>假设第二项的<img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734875907074-c4bc69bb-d11b-4c4d-964a-462e99e4d6a3.png">有：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40781129/1734875982235-d75a48db-a21d-41c0-9887-95c5c9dfab6a.png"></p>
<p>通过再参数化，可用梯度下降法来学习参数</p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">Fetyloi</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://fetyloi.netlify.app/2025/03/21/post2/">https://fetyloi.netlify.app/2025/03/21/post2/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">Fetyloi</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
                                    <span class="chip bg-color">神经网络</span>
                                </a>
                            
                                <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">深度学习</span>
                                </a>
                            
                                <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">机器学习</span>
                                </a>
                            
                                <a href="/tags/%E8%92%B2%E5%85%AC%E8%8B%B1%E4%B9%A6/">
                                    <span class="chip bg-color">蒲公英书</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    
    <script src="https://giscus.app/client.js"
        data-repo="Cavalier-JR/Cavalier-JR.github.io"
        data-repo-id="R_kgDOOLp0RA"
        data-category="Announcements"
        data-category-id="DIC_kwDOOLp0RM4CotJt"
        data-mapping="title"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light_protanopia"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2025/03/22/post3/">
                    <div class="card-image">
                        
                        <img src="/../images/14.png" class="responsive-img" alt="论文阅读：两篇关于LLM Agents利用漏洞的能力研究论文">
                        
                        <span class="card-title">论文阅读：两篇关于LLM Agents利用漏洞的能力研究论文</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" class="post-category">
                                    论文阅读
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                    <a href="/tags/%E6%BC%8F%E6%B4%9E/">
                        <span class="chip bg-color">漏洞</span>
                    </a>
                    
                    <a href="/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                    <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">
                        <span class="chip bg-color">大模型</span>
                    </a>
                    
                    <a href="/tags/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/">
                        <span class="chip bg-color">网络安全</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2025/03/20/post1/">
                    <div class="card-image">
                        
                        <img src="/../images/5.png" class="responsive-img" alt="VideoCaptioner字幕生成与翻译相关配置与cookie问题解决">
                        
                        <span class="card-title">VideoCaptioner字幕生成与翻译相关配置与cookie问题解决</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%B7%A5%E5%85%B7%E8%AE%BE%E7%BD%AE/" class="post-category">
                                    工具设置
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E5%B7%A5%E5%85%B7/">
                        <span class="chip bg-color">工具</span>
                    </a>
                    
                    <a href="/tags/github/">
                        <span class="chip bg-color">github</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('30')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'CODE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: Fetyloi&#39;s Blog<br />'
            + '文章作者: Fetyloi<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('1'),
            headingSelector: 'h1, h2, h3, h4, h5, h6'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4, h5, h6').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>






    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="13473505814"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='list'
                   preload='auto'
                   volume='0.1'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    
    <div class="container row center-align" style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2025</span>
            
            <span id="year">2025</span>
            <a href="/about" target="_blank">Fetyloi</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">28.3k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <span id="sitetime">载入运行时间...</span>
            <script>
                function siteTime() {
                    var seconds = 1000;
                    var minutes = seconds * 60;
                    var hours = minutes * 60;
                    var days = hours * 24;
                    var years = days * 365;
                    var today = new Date();
                    var startYear = "2025";
                    var startMonth = "3";
                    var startDate = "20";
                    var startHour = "0";
                    var startMinute = "0";
                    var startSecond = "0";
                    var todayYear = today.getFullYear();
                    var todayMonth = today.getMonth() + 1;
                    var todayDate = today.getDate();
                    var todayHour = today.getHours();
                    var todayMinute = today.getMinutes();
                    var todaySecond = today.getSeconds();
                    var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                    var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                    var diff = t2 - t1;
                    var diffYears = Math.floor(diff / years);
                    var diffDays = Math.floor((diff / days) - diffYears * 365);
                    var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
                    var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) /
                        minutes);
                    var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours -
                        diffMinutes * minutes) / seconds);
                    if (startYear == todayYear) {
                        document.getElementById("year").innerHTML = todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffDays + " 天 " + diffHours +
                            " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    } else {
                        document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffYears + " 年 " + diffDays +
                            " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    }
                }
                setInterval(siteTime, 1000);
            </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Cavalier-JR" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:j.r@ncepu.edu.cn" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=895484845" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 895484845" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!" title="返回顶部">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>

    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

	
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    


    <!-- 白天和黑夜主题 -->
<!-- <div class="sum-moon-box">
    <a class="btn-floating btn-large waves-effect waves-light" onclick="switchNightMode()" title="切换主题" >
      <i id="sum-moon-icon" class="fas fa-moon" style="width:48px; height:48px; font-size: 28px;"></i>
    </a>
  </div> -->
  
  <script>
    function switchNightMode() {
      $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
          $('body').hasClass('DarkMode') 
          ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
          : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
          setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
              $(this).remove()
            })
          }, 2e3)
        })
    }
  </script>

<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>
    <!-- /* 模式判断 */
    <script>
        /* 模式判断 */
        if (localStorage.getItem('isDark') === '1') {
            document.body.classList.add('DarkMode');
            $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')
        } else {
            document.body.classList.remove('DarkMode');
            $('#sum-moon-icon').removeleClass("fa-sun").addClass('fa-moon')
        }
    </script> -->
   
    <!-- 下面这个是鼠标拖尾，想用的时候取消注释就可以 -->
    <!-- <script src="/js/cursor.js"></script> -->
</body>

</html>
